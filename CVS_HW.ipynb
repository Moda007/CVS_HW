{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVS_HW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul0rvDQ2ZtmZ",
        "colab_type": "text"
      },
      "source": [
        "# **Computer Vision Systems Homework**\n",
        "\n",
        "Hi all!\n",
        "\n",
        "In the spirit of #StayAtHome I tried to assemble a homework project without leaving the house. So, if this looks a little DIY, that's why. So without further ado:\n",
        "\n",
        "## **Welcome to Cactusville!**\n",
        "\n",
        "Cactusville is a small town populated by - you guessed correcty - cacti. Since it is a rapidly developing village, they are considering to use self-driving vehicles in their hometown. Your job as a computer vision maestro is to develop the required detection methods.\n",
        "\n",
        "## The setting\n",
        "\n",
        "Cactusville is quite unique in the sense that the entire surface of the town is covered in blue tablecloth. The exact colour and pattern of the cloth may vary slightly.\n",
        "\n",
        "By-and large there are 3 different objects of interest:\n",
        "\n",
        "* **Cacti:** These are the inhabitants of the village, so self-driving cars must be able to detect them to avoid hitting a cactus. Cacti have four basic sub-types: ***Happy***, ***Sad***, ***Angry*** and ***Evil***\n",
        "* **Vehicles:** These are other vechiles you should also avoid colliding with. There are 3 vehicles in Cactusville: An ***SUV***, a ***truck***, and an ***airplane***.\n",
        "* **Traffic Signs:** There are several signs placed all around the town, often multiple ones on a single stand. There are 55 different traffic sign classes, which are not listed here for the sake of brevity.\n",
        "\n",
        "## Tasks\n",
        "\n",
        "The people of Cactusville provided 4 videos for you to develop your algorithms with. Each video consists of several RGB and corresponding depth frames, which are found in the '*rgb*' and '*depth*' subfolders of the video. They are ordered numerically. The depth image is a single-channel, 16-bit image, where the pixel value is the distance of that pixel from the camera in **mm**.\n",
        "\n",
        "The videos also contain a **calibration.yaml** file, which contains the intrinsic parameters of the camera. These are the same for all videos used, so feel free to hardcode the important values into your program.\n",
        "\n",
        "Your team has to complete the following tasks:\n",
        "\n",
        "1.   **Traditional Vision:** Create an algorithm to accurately detect and classify the 3 objects of interest (Cactus, Vehicle, Traffic Sign). You don't have to determine the subclass at this point.\n",
        "2.   **Deep Learning:** Use a deep learning algorithm to classify traffic signs. The package provided includes a training and validation database of 32x32 RGB images.\n",
        "3.   **3D Vision:** Determine the 3D positions of the object of interest relative to the camera. Use the center of an object's bounding box to determine the position on the image.\n",
        "\n",
        "## Hardcore Tasks\n",
        "\n",
        "There are also 3 hardcore tasks for those who like challenges. These aren't particularly difficult, but they take more work and require you to go a little bit beyond the scope of the practicals.\n",
        "\n",
        "1.   **Traditional Vision:** Determine the subclasses of Cacti and Vehicles\n",
        "2.   **Deep Learning:** Of the 55 possible traffic signs, 3 are missing from the training and test datasets. ('*X - Priority*', '*X - Turn left*', '*X - Turn right*') As a result, the neural net trained in task 2 will not be able to classify them properly. Extend your neural network to classify these as well.\n",
        "3.   **3D Vision:** Determine the absolute pose (4x4 transformation matrix) of the camera as it moves throughout the video. You can safely assume that the pose in the first frame of every video is the identity matrix.\n",
        "\n",
        "## Evaluation and Score\n",
        "\n",
        "The basic package also contains annotations (correct answers) in the file **annotations.pickle** and a small python script **evaluate.py** you can use to measure the performance of your algorithm. \n",
        "\n",
        "Your homework score will be computed using the same script, albeit on 2 secret videos that you were not provided. The reason for this is to make sure that your algorithm works in new situations as well. The secret videos use the same 2 tablecloths and 3 vehicles, but the traffic signs and the cacti may be different. Not to mention the illumination.\n",
        "\n",
        "The tasks will be evaluated using the following metrics:\n",
        "\n",
        "* Task 1 - **Average Precision** (AP): This metric is simply the average of **Recall** (nCorrect / nObject) and **Precision** (nCorrect / nPrediction).\n",
        "* Tasks 1 HC, 2 and 2 HC - **Classification accuracy**\n",
        "* Tasks 3 and 3 HC - **RBF error**: This is simple the squared error between the prediction and the correct answer transformed by an RBF (Radial Basis Function) kernel. This means that a perfect answer has a score of 1, a bad answer will result in a score close to 0.\n",
        "\n",
        "### **Answer format**\n",
        "\n",
        "The evaluation function takes a single argument: A dictionary that containes your predictions. On the top level this dictionary should look like this:\n",
        "\n",
        "```python\n",
        "myAnswers = {\n",
        "    'video1/rgb/1.jpg' : <<Predictions for the image>>,\n",
        "    'video1/rgb/2.jpg' : <<Predictions for the image>>,\n",
        "    ...\n",
        "    'video4/rgb/10.jpg' : <<Predictions for the image>>,\n",
        "}\n",
        "```\n",
        "It is important that the dictionary key contains the video path, since two videos might have image files with the same name. Also, include all images from all videos in the file (even if you have no predictions), since the evaluation function will look for them! The order of the images does not matter.\n",
        "\n",
        "A prediction for a single image should also be a dictionary with the following format:\n",
        "```python\n",
        "myPred = {\n",
        "    'poses' : [t_11, t12_, t_13, t_14, ..., t_33, t_34],\n",
        "    'objects' : [obj_1, obj_2, ... obj_n]\n",
        "}\n",
        "```\n",
        "The key `poses` contains the first three rows of the transformation matrix (the fourth row is always `[0 0 0 1]`). The key `objects` is a list, each element containing a single object prediction. The order of predictions does not matter. A single object prediction is also a list, containing the following elements:\n",
        "```python\n",
        "myObjects = [u, v, w, h, classInd, subClassInd, x, y, z]\n",
        "```\n",
        "\n",
        "* `(u, v)` are the center coordinates of the object's bounding box, while `(w, h)` are the width and height parameters. All four are expected in pixels @640x480 resolution.\n",
        "* `(x, y, z)` are the 3D coordinates of the object relative to the camera. They are expected in **meters**.\n",
        "* `classInd` is the index of the object class in the list `className` (see below). It is between 0 and 2.\n",
        "* `subClassInd` is the index of the subclass in the appropriate list in `subclassNames` (again, see below). It is between [0-54] for traffic signs, [0-2] for vehicles and [0-3] for cacti.\n",
        "\n",
        "## Rules\n",
        "\n",
        "Here are some important rules and guidelines you have to follow:\n",
        "\n",
        "*   This work is to be done in groups of 3 or 4 people. You can do it with less if you feel confident, but not more.\n",
        "*   Forming/finding a group is your job. Once you have one, 1 person from the group shold write me a message on teams with the names and neptun codes of the members.\n",
        "*   If you can't find a group by Sunday, write me and I'll formulate groups with the remaining people.\n",
        "*   The deadline for the submission is Friday midnight on the 14th week. You can make a late submission until the next Sunday midnight.\n",
        "*   You can opt out of the homework. In this case you will beed to take the midterm exam. This will be done via teams video chat (oral exam). If you want to take this option, write me a message by Sunday.\n",
        "*   To pass the homework, you will have to submit a working solution for the 3 basic tasks. The quality of your predictions has to be significantly better than what is achievable by random guessing.\n",
        "\n",
        "### Offered final grade\n",
        "\n",
        "To qualify for the offered final grade (and to skip the exams), you have to complete at least one of the hardcore tasks. What this final grade will be depends on the quality of the predictions. \n",
        "\n",
        "I cannot specify the criteria exactly at this time, since I don't know how easy/hard this homework is yet. I will, however adhere to the following guidelines:\n",
        "\n",
        "*   I'm planning to offer Good (4) and Excellent (5) final grades.\n",
        "*   Those, who completed all 3 hardcore tasks with high quality are gonna get a 5\n",
        "*   Those, who completed at least 2 hardcore tasks with high quality are gonna get **at least** a 4\n",
        "*   'High quality' is undefined to create a situation in which teams compete\n",
        "*   Also, I want to avoid two situations: a., where the criterion is so hard that only a few people manage to get an offered grade; and b., where it is so easy that everyone gets one.\n",
        "*   My goal is that about 40-50% of all students would get an offered grade, 15-20% getting 5, and 25-30% getting 4. These goals are might change if way more people opt out of homework than I expect.\n",
        "\n",
        "### Ethics\n",
        "\n",
        "Copying entire solutions from online sources or each other is plagiarism, and it will be checked using automated tools. There are things that are perfectly okay, such as:\n",
        "*   Copying small snippets (a few lines) from the OpenCV/PyTorch tutorials or stackoverflow, etc.\n",
        "*   Appropriating code from the practicals (you can copy the entire thing), especially the deep learning one.\n",
        "*   Since what is okay and what isn't is a bit subjective, if you are unsure, ask me.\n",
        "\n",
        "## So, how should we do this?\n",
        "\n",
        "So, how can you do this homework, especially if you haven't done things like this before? Here are a few tips:\n",
        "\n",
        "### Environment\n",
        "\n",
        "For development IDE the easiest is to just use Google Colab. To do this you just have to solve the homework inside this notebook. This is the simplest solution, although it has one drawback: the colab notebook has limited debugging capabilities.\n",
        "\n",
        "If you want something more powerful, I recommend the [PyCharm](https://www.jetbrains.com/pycharm/) IDE, which is a free and pretty powerful Python development tool.\n",
        "\n",
        "If you are planning to use PyCharm on Windows, you need to install a Python distribution, since Windows still doesn't come with one (it's 20 effing 20, Microsoft!). I recommend [Anaconda](https://www.anaconda.com/distribution/). Make sure you use Python 3.x and not 2.7.\n",
        "\n",
        "[Here's a tutorial on how to set it up.](https://www.youtube.com/watch?v=e53lRPmWrMI)\n",
        "\n",
        "### Collaboration within the team\n",
        "\n",
        "Since I would strongly discourage teams to collaborate physically in the current situation, I would recommend some methods for remote collaboration.\n",
        "\n",
        "* First of all, use Teams or similar methods to communicate.\n",
        "* Second, use git or a similar version control tool to handle multiple team members working on the same project. \n",
        "* I strongly recommend creating a private repository for your homework on [Github](https://github.com/) (since you can add exactly 3 collaborators - including you that's a 4 person team). There, you can also create issues and other nice-to-have features to track you development. Getting some experience with version control is an absolute must for any engineer anyways.\n",
        "\n",
        "Here's a tutorial for git for those who never used something like this before.\n",
        "\n",
        "To use git from a GUI, I recommend [SmartGit](https://www.syntevo.com/smartgit/) or [Git Extensions](http://gitextensions.github.io/).\n",
        "\n",
        "**ProTip:** If you use a Colab notebook, make sure to clear the output cells (especially figures and images) before you commit. Otherwise you'll litter in your repository.\n",
        "\n",
        "[Here is an introduction to git](https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/)\n",
        "\n",
        "### Making a submission\n",
        "\n",
        "You can make a submission at the appropriate page in the edu portal. The results and leaderboard will also be published here. The results are evaluated around 8pm (CET), so it's pointless to make multiple submission per day.\n",
        "\n",
        "**Note**: Your submission should be runnable from Colab or PyCharm (if you used any custom libraries, please note it), and it must include the trained neural network model file from task 2. Also, make sure that only the code required for evaluation is ran (you can use a control variable to skip training code).\n",
        "\n",
        "### Further resources\n",
        "\n",
        "[Python tutorials](https://docs.python.org/3/tutorial/)\n",
        "\n",
        "[OpenCV tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)\n",
        "\n",
        "[PyTorch tutorials](https://pytorch.org/tutorials/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgWfyt5SdmUn",
        "colab_type": "text"
      },
      "source": [
        "# Solution\n",
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpJi3x8AZtEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Homework dataset\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/HW.zip\n",
        "!unzip -qq HW.zip\n",
        "!rm HW.zip\n",
        "\n",
        "# Traffic Sign Classification set\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/trafficSignsHW.zip\n",
        "!unzip -qq trafficSignsHW.zip\n",
        "!rm trafficSignsHW.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSTAlH8td3eD",
        "colab_type": "text"
      },
      "source": [
        "## Folder example\n",
        "\n",
        "Get all subfolders in a directory\n",
        "\n",
        "```\n",
        "import os\n",
        "myFolderList = [f.path for f in os.scandir(path) if f.is_dir()]\n",
        "```\n",
        "\n",
        "Get all files with extension in a directory\n",
        "\n",
        "```\n",
        "import glob\n",
        "import re\n",
        "\n",
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "    return sorted(l, key = alphanum_key)\n",
        "\n",
        "names = sorted_nicely(glob.glob1(path, \"*.extension\"))\n",
        "```\n",
        "\n",
        "### Class names\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHMtqb8G7Lwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classNames = ['traffic sign', 'vehicle', 'cactus']\n",
        "subclassNames = [\n",
        "    ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking', 'Road works', 'Roundabout', 'Slippery road', 'Stop', 'Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)', 'Wild animals', 'X - Priority', 'X - Turn left', 'X - Turn right'],\n",
        "    ['SUV','truck','plane'],\n",
        "    ['happy','sad','angry','evil']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVZdwqSb7dyZ",
        "colab_type": "text"
      },
      "source": [
        "### Display the first images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNjLlbEl7hv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "colors = [(0,0,255),(255,0,255),(0,255,0)]\n",
        "\n",
        "def drawBBs(BBs, img):\n",
        "    img = cv2.resize(img, (1280, 960))\n",
        "    for BB in BBs:\n",
        "        u = BB[0]*2\n",
        "        v = BB[1]*2\n",
        "        w = BB[2]*2\n",
        "        h = BB[3]*2\n",
        "        c = BB[4]\n",
        "        sc = BB[5]\n",
        "        x = BB[6]\n",
        "        y = BB[7]\n",
        "        z = BB[8]\n",
        "        s = (u - w // 2, v - h // 2)\n",
        "        e = (u + w // 2, v + h // 2)\n",
        "        cv2.rectangle(img, s, e, colors[c], 1)\n",
        "        tl = (s[0], s[1]+15)\n",
        "        bl = (s[0], e[1]-5)\n",
        "        cv2.putText(img,subclassNames[c][sc],tl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.75,colors[c])\n",
        "        coords = \"(%.2f, %.2f, %.2f)\" % (x,y,z)\n",
        "        cv2.putText(img,coords,bl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.65,colors[c])\n",
        "    \n",
        "    return img\n",
        "\n",
        "import pickle\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "#This way it doesn't try to open a window un the GUI - works in python notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Read images\n",
        "img = cv2.imread(\"HW/g1/rgb/1.jpg\")\n",
        "depth = cv2.imread(\"HW/g1/depth/1.png\", -1)\n",
        "\n",
        "# Read annotations\n",
        "file = open('HW/annotations.pickle','rb')\n",
        "annotations = pickle.load(file)\n",
        "\n",
        "# Visualization\n",
        "depth = depth / 5000.0\n",
        "img = drawBBs(annotations[\"HW/g1/rgb/1.jpg\"][\"objects\"], img)\n",
        "img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Figure with subplots\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_rgb)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(depth,cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOTJfoULdtTy",
        "colab_type": "text"
      },
      "source": [
        "# Your Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVfLK9bz_EoX",
        "colab_type": "text"
      },
      "source": [
        "## Chores: Clone project repository, and Import libs, assets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFhCTklCT9MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf *\n",
        "!git clone https://github.com/Moda007/CVS_HW.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X2SyWik_OtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import pickle\n",
        "import time\n",
        "from datetime import datetime\n",
        "import imutils\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Ql0P-d9axu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import DL libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n",
        "import keras\n",
        "from numpy import loadtxt\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ea4C_9B_PGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import assets\n",
        "Traffic_Signs_Folder_Path = '/content/trafficSignsHW/'\n",
        "DL_MODEL_PATH = '/content/CVS_HW/savedModels/'\n",
        "DL_MODEL_2_FILE = 'model_task02.h5'\n",
        "DL_MODEL_2HC_FILE = 'model_task02HC.h5'\n",
        "\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/HW.zip\n",
        "!unzip -qq HW.zip\n",
        "!rm HW.zip\n",
        "\n",
        "# Traffic Sign Classification set\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/trafficSignsHW.zip\n",
        "!unzip -qq trafficSignsHW.zip\n",
        "!rm trafficSignsHW.zip\n",
        "\n",
        "# Missing Traffic Sign Classification set\n",
        "!mkdir /content/trafficSignsExtra\n",
        "!unzip -qq /content/CVS_HW/TrafficTrain.zip -d /content/trafficSignsExtra\n",
        "!unzip -qq /content/CVS_HW/TrafficTest.zip -d /content/trafficSignsExtra\n",
        "\n",
        "!cp -R /content/trafficSignsExtra/TrafficTrain/* /content/trafficSignsHW/trainFULL\n",
        "!cp -R /content/trafficSignsExtra/TrafficTest/* /content/trafficSignsHW/testFULL\n",
        "\n",
        "# !mkdir /content/CVS_HW/savedModels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ysXLit5ZpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get run time\n",
        "dateTimeObj = datetime.now()\n",
        "run_datetime = str(dateTimeObj.month) + str(dateTimeObj.day) + str(dateTimeObj.hour+2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBgC-GDkAA7k",
        "colab_type": "text"
      },
      "source": [
        "## Sub-functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKbChfrVGJTz",
        "colab_type": "text"
      },
      "source": [
        "### Asset processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy09RNPFTGlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = \"/content\"\n",
        "videos_path = \"HW\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldy-UnGZThJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_names_nicely(names):\n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "    return sorted(names, key = alphanum_key)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg7sb3CXHDWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_folders_to_predict():\n",
        "  \"\"\" Get main folder for all videos to be processed.\"\"\"\n",
        "  # folder_paths = ['/content/HW/g1'] # mock line\n",
        "  path = base_path + '/' + videos_path\n",
        "  \n",
        "  folder_paths = [f.path for f in os.scandir(path) if f.is_dir()]\n",
        "  return folder_paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m19TNQwnSZyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_img_seq_nr(path):\n",
        "  \"\"\" Get sorted list of image sequence numbers/file names without extension.\"\"\"\n",
        "  seqs = [os.path.basename(file)[:-4] for file in glob.glob(path + '/**/*.jpg')]\n",
        "  seqs_sorted = sort_names_nicely(seqs)\n",
        "  return seqs_sorted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gGjnDCsyIPBp",
        "colab": {}
      },
      "source": [
        "classNames = ['traffic sign', 'vehicle', 'cactus']\n",
        "subclassNames = [\n",
        "    ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking', 'Road works', 'Roundabout', 'Slippery road', 'Stop', 'Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)', 'Wild animals', 'X - Priority', 'X - Turn left', 'X - Turn right'],\n",
        "    ['SUV','truck','plane'],\n",
        "    ['happy','sad','angry','evil']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3_slhdNGPF7",
        "colab_type": "text"
      },
      "source": [
        "### Object processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2okcpOx_cS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_objects_in_video(video_path):\n",
        "  # images = [\"1\",\"159\"] # mock line\n",
        "  images = get_img_seq_nr(video_path)\n",
        "  objects_in_video = {}\n",
        "  visualized_list = []\n",
        "\n",
        "  for image in images:\n",
        "    path_len = len(base_path) # path position for extracting image key\n",
        "    img_key = video_path[path_len+1:] + \"/\" + \"rgb/\" + image + \".jpg\"\n",
        "    img_path = video_path + \"/\" + \"rgb/\" + image + \".jpg\"\n",
        "    img_depth_path = video_path + \"/\" + \"depth/\" + image + \".png\"\n",
        "\n",
        "    img_orig = cv2.imread(img_path)\n",
        "    img_orig_depth = cv2.imread(img_depth_path,-1)\n",
        "    print(img_depth_path)\n",
        "\n",
        "    objects_in_image, image_proc_steps = predict_objects_in_image(img_orig, img_orig_depth,image)\n",
        "    objects_in_video[img_key] = objects_in_image\n",
        "    visualized_list.append(image_proc_steps)\n",
        "\n",
        "  return objects_in_video, visualized_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYFAms8H9Q_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_objects_in_image(rgb_img, depth_img, seq_nr):\n",
        "  \"\"\" Get predictions for a single image based on the colored and the depth image.\"\"\"\n",
        "  poses = get_poses(seq_nr)\n",
        "  objects, image_proc_steps = detect_objects_in_image(rgb_img, depth_img) # [u, v, w, h, classInd, subClassInd, x, y, z, leftTopX, leftTopY]\n",
        "  object_list = []\n",
        "\n",
        "  for obj in objects:\n",
        "    cropped_obj, cropped_img_wo_bg = crop_obj(rgb_img.copy(), image_proc_steps[3].copy(), obj)\n",
        "    obj['classInd'], extra_params = classify_ooi(cropped_obj,cropped_img_wo_bg, rgb_img, obj)\n",
        "    obj.pop('leftTopX')\n",
        "    obj.pop('leftTopY')\n",
        "\n",
        "    if obj['classInd'] != 0:\n",
        "      if obj['classInd'] == 2:\n",
        "        obj['subClassInd'] = detect_vehicles_cactus_subclass(cropped_obj, obj['classInd'])\n",
        "      else:\n",
        "        obj['subClassInd'] = extra_params\n",
        "      obj['x'], obj['y'], obj['z'] = get_camera_coords(depth_img, obj['u'], obj['v'])\n",
        "      obj_to_append = list(obj.values())\n",
        "    else:\n",
        "      if len(extra_params) > 0:\n",
        "        for sign in extra_params:\n",
        "          sign.pop('leftTopX')\n",
        "          sign.pop('leftTopY')\n",
        "          sign['x'], sign['y'], sign['z'] = get_camera_coords(depth_img, sign['u'], sign['v'])\n",
        "          obj_to_append = list(sign.values())\n",
        "      else:\n",
        "        obj['x'], obj['y'], obj['z'] = get_camera_coords(depth_img, obj['u'], obj['v'])\n",
        "        obj_to_append = list(obj.values())\n",
        "    \n",
        "    object_list.append(obj_to_append)\n",
        "    \n",
        "  prediction = { 'poses' : poses, 'objects' : object_list }\n",
        "\n",
        "  return prediction, image_proc_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udSRJXu85Q77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_obj(img, eroded_img, obj_param):\n",
        "  \"\"\" crop objects out from the original image depending on thier parameters.\"\"\"\n",
        "  leftTopX = obj_param['leftTopX']\n",
        "  leftTopY = obj_param['leftTopY']\n",
        "  w = obj_param['w']\n",
        "  h = obj_param['h']\n",
        "\n",
        "  w0 = leftTopX\n",
        "  w1 = leftTopX + w\n",
        "  h0 = leftTopY\n",
        "  h1 = leftTopY + h\n",
        "\n",
        "  cropped_img = img[h0:h1 , w0:w1]\n",
        "  cropped_eroded_img = eroded_img[h0:h1 , w0:w1]\n",
        "\n",
        "  black_pixels_mask = np.any(cropped_eroded_img != [0, 0, 0], axis=-1)  \n",
        "  cropped_img_wo_bg = cropped_img.copy()\n",
        "  cropped_img_wo_bg[black_pixels_mask] = [0,0,0]\n",
        "\n",
        "  return cropped_img, cropped_img_wo_bg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y5EwOCN0sYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_resized_image(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
        "    \"\"\" Helper function to find the best image size for template matching.\"\"\"\n",
        "    dim = None\n",
        "    (h, w) = image.shape[:2]\n",
        "\n",
        "    if width is None and height is None:\n",
        "        return image\n",
        "\n",
        "    if width is None:\n",
        "        # Calculate the ratio of the height and construct the dimensions\n",
        "        r = height / float(h)\n",
        "        dim = (int(w * r), height)\n",
        "    else:\n",
        "        # Calculate the ratio of the width and construct the dimensions\n",
        "        r = width / float(w)\n",
        "        dim = (width, int(h * r))\n",
        "\n",
        "    # Return the resized image\n",
        "    return cv2.resize(image, dim, interpolation=inter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqJnXPFYdo_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fetching_traffic_signs(dataSetType, extra = False):\n",
        "\n",
        "  if extra:\n",
        "    idx_start, idx_end = 52, 55\n",
        "    i = 52\n",
        "  else:\n",
        "    idx_start, idx_end = 0, 52\n",
        "    i = 0\n",
        "\n",
        "  main_path = Traffic_Signs_Folder_Path\n",
        "  sub_path = dataSetType + 'FULL/'\n",
        "  \n",
        "  x_data = np.array(list())\n",
        "  y_data = np.array(list())\n",
        "  for name in subclassNames[0][idx_start : idx_end]:\n",
        "    imgs = glob.glob(main_path + sub_path + name + '/*.jpg')\n",
        "    x = np.array([np.array(Image.open(fname)) for fname in imgs])\n",
        "    y = np.full(len(imgs), i)\n",
        "    i += 1\n",
        "    if x_data.size == 0:\n",
        "      x_data = x.copy()\n",
        "      y_data = y.copy()\n",
        "    else:\n",
        "      x_data = np.vstack((x_data, x))\n",
        "      y_data = np.hstack((y_data, y))\n",
        "\n",
        "\n",
        "  return x_data, y_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKIUsSDBGiui",
        "colab_type": "text"
      },
      "source": [
        "### 1 Traditional vision\n",
        ">Create an algorithm to accurately detect and classify the 3 objects of interest (Cactus, Vehicle, Traffic Sign).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8zMiz7Lwb0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_objects_in_image(rgb_img, depth_img):\n",
        "  \"\"\" Get bounding boxes/rectangles for objects.\"\"\"\n",
        "  image_proc_steps = []\n",
        "  # ============================================================================\n",
        "  # Create clustered depth image >> Identify background wall\n",
        "  # Input: depth_img\n",
        "  # Output: depth_clustered\n",
        "  # ============================================================================\n",
        "  depth_div = depth_img.copy()/255\n",
        "  depth_norm = depth_div.reshape(depth_div.shape[0]*depth_div.shape[1], -1)\n",
        "\n",
        "  kmeans = KMeans(n_clusters=2, random_state=1).fit(depth_norm)\n",
        "  clustered = kmeans.cluster_centers_[kmeans.labels_]\n",
        "  depth_clusters = clustered.reshape(depth_div.shape[0], depth_div.shape[1], -1)\n",
        "  depth_clustered = depth_clusters.reshape(depth_div.shape[0], depth_div.shape[1])\n",
        "  image_proc_steps.append(depth_clustered)\n",
        "\n",
        "  # ============================================================================\n",
        "  # Create threshold for potential tableclothes >> Identify floor\n",
        "  # Input: rgb_img\n",
        "  # Output: floor_removed\n",
        "  # ============================================================================\n",
        "\n",
        "  shifted_img = cv2.pyrMeanShiftFiltering(rgb_img.copy(), 21, 51) # 21, 51\n",
        "  shifted_img_gray = cv2.cvtColor(shifted_img, cv2.COLOR_BGR2GRAY)\n",
        "  floor_removed = cv2.threshold(shifted_img_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "  image_proc_steps.append(floor_removed)\n",
        "\n",
        "  # ============================================================================\n",
        "  # Remove background and floor >> Get only the interesting areas on the img\n",
        "  # Input: depth_img, rgb_img, depth_clustered\n",
        "  # Output: interesting_spots\n",
        "  # ============================================================================\n",
        "  orig_depth=depth_img.copy()\n",
        "  interesting_spots=rgb_img.copy()\n",
        "\n",
        "  thr = np.unique(depth_clustered*255)[0]\n",
        "  for row in range(orig_depth.shape[0]):\n",
        "    row_stats={}\n",
        "    no_zero = np.asarray([i for i in orig_depth[row] if i != 0])\n",
        "    row_stats['mean']=no_zero.mean()\n",
        "    thresh=row_stats['mean']\n",
        "    for col in range(orig_depth.shape[1]-1, -1, -1):\n",
        "      if(orig_depth[row][col]>thresh or orig_depth[row][col] == 0 or depth_clustered[row][col]*255 > thr or floor_removed[row][col]==255):\n",
        "        interesting_spots[row][col][0]=255\n",
        "        interesting_spots[row][col][1]=255\n",
        "        interesting_spots[row][col][2]=255\n",
        "      else:\n",
        "        interesting_spots[row][col][0]=0\n",
        "        interesting_spots[row][col][1]=0\n",
        "        interesting_spots[row][col][2]=0\n",
        "  image_proc_steps.append(interesting_spots)\n",
        "\n",
        "  # ============================================================================\n",
        "  # Erode image >> Prepare for edge detection\n",
        "  # Input: interesting_spots\n",
        "  # Output: eroded_img_uint8\n",
        "  # ============================================================================\n",
        "  kernel = np.ones((3,3),np.uint8)\n",
        "  to_be_eroded=interesting_spots.copy()\n",
        "  eroded_img=cv2.erode(to_be_eroded,kernel,iterations = 7)\n",
        "  eroded_img_uint8 = eroded_img.astype(np.uint8)\n",
        "  image_proc_steps.append(eroded_img_uint8)\n",
        "\n",
        "  # ============================================================================\n",
        "  # Canny edge detection, Contour search >> Prepare for bounding box detection\n",
        "  # Input: eroded_img_uint8\n",
        "  # Output: img_edges, contours\n",
        "  # ============================================================================\n",
        "  to_be_cannied=cv2.cvtColor(eroded_img_uint8.copy(), cv2.COLOR_BGR2GRAY)\n",
        "  img_edges = cv2.Canny(to_be_cannied,50,120)\n",
        "  contours,hierarchy = cv2.findContours(img_edges, 1, 2)\n",
        "  # contours,hierarchy = cv2.findContours(to_be_contoured,cv2.RETR_LIST,cv2.CHAIN_APPROX_NONE)\n",
        "  # print(len(contours))\n",
        "  image_proc_steps.append(img_edges)\n",
        "\n",
        "  # ============================================================================\n",
        "  # Get bounding boxes based on contours\n",
        "  # Input: rgb_img, contours\n",
        "  # Output: bounding_rects\n",
        "  # ============================================================================\n",
        "  pic_for_rects = rgb_img.copy()\n",
        "\n",
        "  bounding_rects = []\n",
        "  for cnt in contours:\n",
        "    x,y,w,h = cv2.boundingRect(cnt)\n",
        "    u=x+w//2\n",
        "    v=y+h//2\n",
        "    duplicate = is_duplicate(bounding_rects,u,v,w,h,x,y)\n",
        "    \n",
        "    if(w < 300 and w*h>650 and w>21 and h>21 and h <300 and not duplicate):\n",
        "      rect = {\"u\":u,\"v\":v,\"w\":w,\"h\":h,\"classInd\":0,\"subClassInd\":0,\"x\":0,\"y\":0,\"z\":0,\"leftTopX\":x,\"leftTopY\":y}\n",
        "      cv2.rectangle(pic_for_rects, (x, y), (x+w, y+h), (0,255,0), 2)\n",
        "      bounding_rects.append(rect)\n",
        "    # cv2.drawContours(pic_for_rects_cont, contours, -1, (0,255,0), 3)\n",
        "\n",
        "  # reminder: this picture is not the final box after the duplicate switch\n",
        "  image_proc_steps.append(pic_for_rects)\n",
        "\n",
        "  # bounding_rects = [{\"u\":129,\"v\":376,\"w\":183,\"h\":126,\"classInd\":0,\"subClassInd\":0,\"x\":0,\"y\":0,\"z\":0},\n",
        "  #         {\"u\":595,\"v\":280,\"w\":87,\"h\":147,\"classInd\":0,\"subClassInd\":0,\"x\":0,\"y\":0,\"z\":0}]\n",
        "  return bounding_rects, image_proc_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXHjBgjcM6y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_duplicate(item_list, u,v,w,h,x,y):\n",
        "  \"\"\" Remove duplicate rectangles in list based on u,v,w,h,x,y params\"\"\"\n",
        "  for item in item_list:\n",
        "      overlap = has_overlap(x,y,x+w,y+h, item[\"leftTopX\"], item[\"leftTopY\"], item[\"leftTopX\"]+item[\"w\"], item[\"leftTopY\"]+item[\"h\"])\n",
        "      if(overlap!=None):\n",
        "        if( w*h < item[\"w\"]*item[\"h\"] ):\n",
        "          small_area = w*h\n",
        "        else:\n",
        "          small_area = item[\"w\"]*item[\"h\"]\n",
        "          item[\"u\"],item[\"v\"],item[\"w\"],item[\"h\"],item[\"leftTopX\"],item[\"leftTopY\"]=u,v,w,h,x,y\n",
        "        \n",
        "        if small_area == 0:\n",
        "          overlap_ratio = 0\n",
        "        else:\n",
        "          overlap_ratio = overlap/small_area\n",
        " \n",
        "        if(overlap_ratio>0.75):\n",
        "          return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLMkt8rWuI4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def has_overlap(x,y,maxx,maxy, x2, y2, maxx2, maxy2):\n",
        "  dx = min(maxx, maxx2) - max(x, x2)\n",
        "  dy = min(maxy, maxy2) - max(y, y2)\n",
        "  if (dx>=0) and (dy>=0):\n",
        "    return dx*dy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPx4hIXJxFzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify_ooi(obj, obj_wo_bg, rgb_img, obj_params):\n",
        "  \"\"\" Classification of object of interests (Cacti, Vehicle, Traffic Sign)\"\"\"\n",
        "  is_cacti = check_if_cacti(obj) \n",
        "  vehicle_type = check_if_vehicle(obj_wo_bg)\n",
        "  signs = []\n",
        "  if(is_cacti):\n",
        "    return 2, None\n",
        "  elif(vehicle_type > 0):\n",
        "    return 1, vehicle_type\n",
        "  else:\n",
        "    signs = check_traffic_signs(obj, obj_params)\n",
        "    return 0, signs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlhiUv8IuV4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_if_cacti(obj):\n",
        "  \"\"\" Return true, if template matching gets good enough results\"\"\"\n",
        "  whole_img=obj.copy()\n",
        "  cacti_face=cv2.imread('/content/CVS_HW/happy_warped_sample.jpg')\n",
        "\n",
        "  template = cv2.cvtColor(cacti_face, cv2.COLOR_BGR2GRAY)\n",
        "  template = cv2.Canny(template, 50, 200)\n",
        "  (tH, tW) = template.shape[:2]\n",
        "\n",
        "  whole_gray = cv2.cvtColor(whole_img, cv2.COLOR_BGR2GRAY)\n",
        "  found = None\n",
        "\n",
        "  for scale in np.linspace(0.1, 3.0, 20)[::-1]:\n",
        "    resized = get_resized_image(whole_gray, width=int(whole_gray.shape[1] * scale))\n",
        "    ratio = whole_gray.shape[1] / float(resized.shape[1])\n",
        "\n",
        "    # Stop if template image size is larger than resized image\n",
        "    if resized.shape[0] < tH or resized.shape[1] < tW:\n",
        "        break\n",
        "\n",
        "    # Detect edges in resized image and apply template matching\n",
        "    canny = cv2.Canny(resized, 50, 200)\n",
        "    detected = cv2.matchTemplate(canny, template, cv2.TM_CCOEFF)\n",
        "    (_, max_val, _, max_loc) = cv2.minMaxLoc(detected)    \n",
        "\n",
        "    # Keep track of correlation value\n",
        "    # Higher correlation means better match\n",
        "    if(found is None or max_val > found[0]):\n",
        "        found = (max_val, detected, ratio)\n",
        "\n",
        "  if(found is None):\n",
        "    return False\n",
        "\n",
        "  (_, rescoord, r) = found\n",
        "  threshold = 5490000\n",
        "  loc = np.where( rescoord >= threshold)\n",
        "\n",
        "  if(len(loc[0]) > 0):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwVCrA82urrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_if_vehicle(obj_wo_bg):\n",
        "\n",
        "  image = obj_wo_bg.copy()\n",
        "\n",
        "  boundaries_red = [([17, 15, 120], [50, 56, 250])]\n",
        "  lower_red = np.array(boundaries_red[0][0], dtype = \"uint8\")\n",
        "  upper_red = np.array(boundaries_red[0][1], dtype = \"uint8\")\n",
        "  mask_red = cv2.inRange(image, lower_red, upper_red)\n",
        "  output_red = cv2.bitwise_and(image, image, mask = mask_red)\n",
        "\n",
        "  count_red = 0\n",
        "  for r in range(output_red.shape[0]):\n",
        "    for c in range(output_red.shape[1]):\n",
        "      if output_red[r][c].all() != 0:\n",
        "        count_red +=1\n",
        "\n",
        "  boundaries_yellow = [([25, 146, 190], [62, 174, 250])]\n",
        "  lower_yellow = np.array(boundaries_yellow[0][0], dtype = \"uint8\")\n",
        "  upper_yellow = np.array(boundaries_yellow[0][1], dtype = \"uint8\")\n",
        "  mask_yellow = cv2.inRange(image, lower_yellow, upper_yellow)\n",
        "  output_yellow = cv2.bitwise_and(image, image, mask = mask_yellow)\n",
        "\n",
        "  count_yellow = 0\n",
        "  for r in range(output_yellow.shape[0]):\n",
        "    for c in range(output_yellow.shape[1]):\n",
        "      if output_yellow[r][c].all() != 0:\n",
        "        count_yellow +=1\n",
        "\n",
        "  if count_red >= 200:\n",
        "    return 2\n",
        "  elif count_yellow > 0:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzWH6TvqrHq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_traffic_signs(obj, p):\n",
        "  sign_rects = []\n",
        " \n",
        "  if(obj.shape[0]>32):\n",
        "    resized = imutils.resize(obj, width=300)\n",
        "    ratio = obj.shape[0] / float(resized.shape[0])\n",
        " \n",
        "    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    thresh = cv2.threshold(blurred, 60, 255, cv2.THRESH_BINARY)[1]\n",
        "    canny = cv2.Canny(thresh,50,120)\n",
        " \n",
        "    contours,hierarchy = cv2.findContours(canny, 1, 2)\n",
        " \n",
        "    pic_for_rects=obj.copy()\n",
        " \n",
        "    for cnt in contours:\n",
        "      x,y,w,h = cv2.boundingRect(cnt)\n",
        "      u=x+w//2\n",
        "      v=y+h//2\n",
        "      \n",
        "      x,y,w,h,u,v=int(x//(1/ratio)),int(y//(1/ratio)),int(w//(1/ratio)),int(h//(1/ratio)),int(u//(1/ratio)),int(v//(1/ratio))\n",
        "\n",
        "      duplicate = is_duplicate(sign_rects,u,v,w,h,x,y)\n",
        "\n",
        "      if(w < 100 and w*h>320 and w>18 and h>18 and h <100 and not duplicate):\n",
        "        rect = {\"u\":u,\"v\":v,\"w\":w,\"h\":h,\"classInd\":0,\"subClassInd\":0,\"x\":0,\"y\":0,\"z\":0,\"leftTopX\":x,\"leftTopY\":y}\n",
        "        sign_rects.append(rect)\n",
        " \n",
        "    for r in sign_rects:\n",
        "      cv2.rectangle(pic_for_rects, (r['leftTopX'], r['leftTopY']), (r['leftTopX']+r['w'], r['leftTopY']+r['h']), (0,255,0), 2)\n",
        "      crop=obj[r['leftTopY']:r['leftTopY']+r['h'],r['leftTopX']:r['leftTopX']+r['w']]\n",
        "      pred = detect_traffic_sign_subClass(crop)\n",
        "      r['subClassInd']=int(pred)\n",
        " \n",
        "      new_u=r['leftTopX']+p['leftTopX']+r['w']//2\n",
        "      new_v=r['leftTopY']+p['leftTopY']+r['h']//2\n",
        "\n",
        "      r['u'],r['v']=new_u, new_v\n",
        " \n",
        "  return sign_rects"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWLEqUx1hXh5",
        "colab_type": "text"
      },
      "source": [
        "### 1 Traditional vision - HC\n",
        ">Create an algorithm to accurately detect and classify the 2 objects of interest (Cactus, Vehicle) into thier subclasses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBk1BZdH384N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_vehicles_cactus_subclass(obj_img, class_idx):\n",
        "  subClass_idx = 10\n",
        "  if class_idx == 1:\n",
        "    subClass_idx = detect_vehicle_subClass(obj_img)\n",
        "  elif class_idx == 2:\n",
        "    subClass_idx = detect_cactus_subClass(obj_img)\n",
        "  return subClass_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOENvVVq-rAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_vehicle_subClass(obj_img):\n",
        "  return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTEobI-S-ztc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_cactus_subClass(obj_img):\n",
        "  return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zytpOX9QGpqa",
        "colab_type": "text"
      },
      "source": [
        "###2 Deep learning\n",
        ">Create a deep learning algorithm to classify traffic signs into 52 provided subclasses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc5hI56ZENBk",
        "colab_type": "text"
      },
      "source": [
        "#### General Methods for DL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_NNeLFyET36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(Xtrain, Xtest, Ytrain, Ytest, no_classes = 52, valid_perc = 0.2):\n",
        "  # Normalizing Data\n",
        "  x_train = Xtrain.astype('float32')\n",
        "  x_test = Xtest.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  #Converting Labels\n",
        "  y_train = tf.keras.utils.to_categorical(Ytrain, no_classes)\n",
        "  y_test = tf.keras.utils.to_categorical(Ytest, no_classes)\n",
        "\n",
        "  #Creating Validation DS\n",
        "  x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, stratify = y_train, test_size=valid_perc, random_state=42)\n",
        "\n",
        "  return x_train, y_train, x_valid, y_valid, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZBp3HuuEWh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_NN(input_shape, no_classes = 52):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(no_classes, activation='softmax'))\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9TA8w0GEecA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createModel(input_shape, no_classes = 52, Summary = True):\n",
        "  opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "  model = create_NN(input_shape, no_classes)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=opt,\n",
        "                      metrics=['accuracy'])\n",
        "  \n",
        "  if Summary:\n",
        "    model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieozVNqEEhRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainModel(Model, batch_size = 32, no_epochs = 10, X_valid = [], Y_valid = [], evaluation = True, X_test = [], Y_test = []):\n",
        "  start_time = time.time()\n",
        "\n",
        "  Model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=no_epochs,\n",
        "          validation_data=(X_valid, Y_valid),\n",
        "          shuffle=True)\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "\n",
        "  print(elapsed_time)\n",
        "\n",
        "  if evaluation:\n",
        "    scores = Model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print('Test loss:', scores[0])\n",
        "    print('Test accuracy:', scores[1])\n",
        "\n",
        "  return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6ew60krc_2O",
        "colab_type": "text"
      },
      "source": [
        "#### Fetching and preparing DL datasets (52 traffic sign classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNceDghRGhhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_classes = 52"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt5mSTD2fl7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = fetching_traffic_signs('train')\n",
        "x_test, y_test = fetching_traffic_signs('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E3aRZB_E6EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train, x_valid, y_valid, x_test, y_test = prepare_data(x_train, x_test, \n",
        "                                                                  y_train, y_test, \n",
        "                                                                  no_classes = no_classes, \n",
        "                                                                  valid_perc = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI4_Ls9ngqBA",
        "colab_type": "text"
      },
      "source": [
        "#### Model Creating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6t8xgpQFZRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "model_task02 = createModel(input_shape, no_classes = no_classes, Summary = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcgMWf-EgyUM",
        "colab_type": "text"
      },
      "source": [
        "#### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bd0aPSzFuDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainModel(model_task02, batch_size = 32, no_epochs = 10, \n",
        "           X_valid = x_valid, Y_valid = y_valid, \n",
        "           evaluation = True,\n",
        "           X_test = x_test, Y_test = y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aIyJZ6ShEQ9",
        "colab_type": "text"
      },
      "source": [
        "#### Model Saving and Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prqe7-2PhHYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save\n",
        "model_task02.save(DL_MODEL_PATH + run_datetime + '_' + DL_MODEL_2_FILE)\n",
        "model_task02.save(DL_MODEL_PATH + DL_MODEL_2_FILE)\n",
        "\n",
        "#Load\n",
        "# model = tf.keras.models.load_model(DL_MODEL_PATH + DL_MODEL_2_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmSs2emChoyh",
        "colab_type": "text"
      },
      "source": [
        "### 2 Deep learning - HC\n",
        ">Extended the deep learning algorithm to classify traffic signs into 55 subclasses (additional three classes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYJrHdOCDn2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_traffic_sign_subClass(obj_img):\n",
        "  # Image Resizing\n",
        "  img = cv2.resize(obj_img, (32,32))\n",
        "\n",
        "  # Model Loading\n",
        "  model = tf.keras.models.load_model(DL_MODEL_PATH + DL_MODEL_2HC_FILE)\n",
        "  \n",
        "  # Predicting\n",
        "  prediction = model.predict(np.array([img])).argmax()\n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2fcr9blj5bW",
        "colab_type": "text"
      },
      "source": [
        "#### Fetching and preparing DL datasets (52 traffic sign classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "14GjRYZ4HCO3",
        "colab": {}
      },
      "source": [
        "no_classes = 55"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kpgamOuOj2La",
        "colab": {}
      },
      "source": [
        "x_train, y_train = fetching_traffic_signs('train')\n",
        "x_test, y_test = fetching_traffic_signs('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpY4_Lf7kDSm",
        "colab_type": "text"
      },
      "source": [
        "#### Fetching and preparing DL datasets (missing 3 traffic sign classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW97Nd1dkIX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_extra, y_train_extra = fetching_traffic_signs('train', extra = True)\n",
        "x_test_extra, y_test_extra = fetching_traffic_signs('test', extra = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUOX5gngkmaI",
        "colab_type": "text"
      },
      "source": [
        "#### Preparing Complete DS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXg3GFtpkoGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.vstack((x_train, x_train_extra))\n",
        "y_train = np.hstack((y_train, y_train_extra))\n",
        "x_test = np.vstack((x_test, x_test_extra))\n",
        "y_test = np.hstack((y_test, y_test_extra))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1zQIshd0HCO9",
        "colab": {}
      },
      "source": [
        "x_train, y_train, x_valid, y_valid, x_test, y_test = prepare_data(x_train, x_test, \n",
        "                                                                  y_train, y_test, \n",
        "                                                                  no_classes = no_classes, \n",
        "                                                                  valid_perc = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I9q1WkHAHCO_"
      },
      "source": [
        "#### Model Creating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxiPZuXzHCPA",
        "colab": {}
      },
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "model_task02HC = createModel(input_shape, no_classes = no_classes, Summary = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T-Nd-MCWHCPC"
      },
      "source": [
        "#### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XmnhAmA9HCPC",
        "colab": {}
      },
      "source": [
        "trainModel(model_task02HC, batch_size = 32, no_epochs = 10, \n",
        "           X_valid = x_valid, Y_valid = y_valid, \n",
        "           evaluation = True,\n",
        "           X_test = x_test, Y_test = y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nwzf2NPq9lKT"
      },
      "source": [
        "#### Model Saving and Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0t32c8nv9lKT",
        "colab": {}
      },
      "source": [
        "# Save\n",
        "model_task02HC.save(DL_MODEL_PATH + run_datetime + '_' + DL_MODEL_2HC_FILE)\n",
        "model_task02HC.save(DL_MODEL_PATH + DL_MODEL_2HC_FILE)\n",
        "\n",
        "#Load\n",
        "# model = tf.keras.models.load_model(DL_MODEL_PATH + DL_MODEL_2HC_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwi4OtGLGtXu",
        "colab_type": "text"
      },
      "source": [
        "### 3 3D Vision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E47BLypHtp5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_camera_coords(depth_img, u, v):\n",
        "  c_matrix = np.array([[606.0482177734375, 0, 314.42626953125, 0.],\n",
        "                     [0, 604.9857788085938, 246.05038452148438, 0.],\n",
        "                     [0, 0, 1., 0.],\n",
        "                     [0., 0., 0., 1.]])\n",
        "\n",
        "  coords = {}\n",
        "\n",
        "  cX = c_matrix[0, 2]\n",
        "  cY = c_matrix[1, 2]\n",
        "  fX = c_matrix[0, 0]\n",
        "  fY = c_matrix[1, 1]\n",
        "\n",
        "  x = ((u - cX) * depth_img[v, u] / fX) / 1000\n",
        "  y = ((v - cY) * depth_img[v, u] / fY) / -1000\n",
        "  z = (depth_img[v, u]) /1000\n",
        "\n",
        "  return x,y,z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0afc3lVyqcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_poses(seq_nr):\n",
        "  if(seq_nr==\"1\"):\n",
        "    return [1.0,0.0,0.0,-0.0,0.0,1.0,0.0,-0.0,0.0,0.0,1.0,-0.0]\n",
        "  else:\n",
        "    return [0.99,0.0,0.0,-0.0,0.0,0.99,0.0,-0.0,0.0,0.0,0.99,-0.0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj524q8S_XlP",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1ZwHMMGHcMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_cactusville_objects(paths):\n",
        "  predictions = {}\n",
        "  rect_img_in_video = []\n",
        "\n",
        "  # paths=['/content/HW/g1', '/content/HW/g2', '/content/HW/g3', '/content/HW/g4'] # mock line\n",
        "\n",
        "  for video_path in paths:\n",
        "    objects_in_video, processed_imgs_for_video = predict_objects_in_video(video_path)\n",
        "    predictions.update(objects_in_video)\n",
        "    rect_img_in_video.append(processed_imgs_for_video)\n",
        "\n",
        "  return predictions, rect_img_in_video"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKj5kHYfEzNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paths = get_folders_to_predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpZ1muU4FsSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cactusville_objects, rect_img_in_video = predict_cactusville_objects(sorted(paths))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APtIgmw5Tmg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(base_path + '/' + videos_path + '/cactusville_objects.pickle', 'wb') as handle:\n",
        "    pickle.dump(cactusville_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj4Bwgd1wIKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.figure(figsize=(80,80))\n",
        "# i = 1\n",
        "# for img in rect_img_in_video[3]:\n",
        "#   for proc_step in img:\n",
        "#     plt.subplot(len(rect_img_in_video[0]),len(img),i)\n",
        "#     plt.imshow(proc_step)\n",
        "#     i+=1\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZmTz3XRdwm1",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This snippet assumes that the contents of the downloaded zip file are in the HW folder, and that your predictions are in a dictionary called predictions that adheres to the format specified above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z4yNvHk0ONm6",
        "outputId": "1754061d-1f90-4948-c40e-b838e2b9a056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "from HW.evaluate import evaluate\n",
        "\n",
        "#file = open('HW/annotations.pickle','rb')\n",
        "file = open('HW/cactusville_objects.pickle','rb')\n",
        "\n",
        "predictions = pickle.load(file)\n",
        "\n",
        "evaluate(predictions)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task 1: 0.3482468616659452\n",
            "Task 1 HC: 0.26685393258426965\n",
            "Task 2: 0.4444444444444444\n",
            "Task 2 HC: 0.8301886792452831\n",
            "Task 3: 0.93356909696051\n",
            "Task 3 HC: 0.6639868095957165\n",
            "Total:  3.487289824496169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JinaKmG55S7B",
        "colab_type": "code",
        "outputId": "2305c17b-6b51-4cef-9703-96664a4d9134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'HW/g1/rgb/1.jpg': {'poses': [1.0, 0.0, 0.0, -0.0, 0.0, 1.0, 0.0, -0.0, 0.0, 0.0, 1.0, -0.0], 'objects': [[129, 393, 183, 99, 1, 2, -0.18357575924815503, -0.14573858159235287, 0.6], [594, 312, 92, 66, 1, 1, 0.31876250451432875, -0.07532604218465798, 0.691], [343, 283, 77, 19, 0, 53, 0.023432366644262032, -0.03035436457529075, 0.497], [415, 248, 51, 23, 0, 53, 0.1445424912890828, -0.0028068677665964797, 0.871], [144, 117, 32, 33, 0, 54, -0.21681221057185796, 0.16446311624383445, 0.771], [401, 102, 35, 37, 0, 52, 0.11256546526942862, 0.18762705997233478, 0.788], [537, 119, 89, 161, 2, 0, 0.38451510781919, 0.21987583386828627, 1.047], [155, 9, 37, 19, 0, 53, -0.0, 0.0, 0.0]]}, 'HW/g1/rgb/33.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[257, 358, 164, 174, 0, 0, -0.05827449817684567, -0.11380269740368501, 0.615], [51, 361, 102, 79, 1, 2, -0.27644517797180806, -0.12084243631033505, 0.636], [239, 239, 63, 63, 0, 53, -0.06633498867112464, 0.006211476503384201, 0.533], [548, 230, 96, 98, 1, 1, 0.33376032575275233, 0.02297514004871023, 0.866], [239, 239, 63, 63, 0, 53, -0.06633498867112464, 0.006211476503384201, 0.533], [324, 209, 60, 38, 0, 0, 0.013774964968003404, 0.05340280124659923, 0.872], [548, 230, 96, 98, 1, 1, 0.33376032575275233, 0.02297514004871023, 0.866], [578, 174, 45, 20, 0, 53, 0.4409942226663312, 0.12076166492485386, 1.014], [64, 122, 35, 23, 0, 54, -0.32726373851477225, 0.16239704796779933, 0.792], [127, 108, 31, 38, 0, 53, -0.3516282404722738, 0.25944954856631103, 1.137], [299, 78, 32, 37, 0, 52, -0.019879468581178457, 0.2169428685906409, 0.781], [452, 94, 83, 151, 2, 0, 0.2397133678715149, 0.2654032733974187, 1.056], [602, 21, 74, 19, 0, 53, 0.4379693652063237, 0.34334940124113117, 0.923], [131, 20, 28, 31, 0, 52, -0.36500739473733507, 0.45061681329068237, 1.206], [16, 25, 33, 24, 0, 53, -0.8735413893195363, 0.648186115900718, 1.774]]}, 'HW/g1/rgb/60.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[134, 293, 86, 29, 0, 54, -0.19767906159165816, -0.05152938427598547, 0.664], [134, 293, 86, 29, 0, 54, -0.19767906159165816, -0.05152938427598547, 0.664], [134, 293, 86, 29, 0, 54, -0.19767906159165816, -0.05152938427598547, 0.664], [404, 231, 98, 77, 1, 1, 0.10405097218448392, 0.017513586392048416, 0.704], [97, 260, 22, 41, 0, 39, -0.202699783090659, -0.013027633080702379, 0.565], [50, 222, 42, 21, 0, 53, -0.22688237688647578, 0.020671890793533185, 0.52], [198, 209, 37, 24, 0, 0, -0.1679017553160312, 0.05352528473569693, 0.874], [404, 231, 98, 77, 1, 1, 0.10405097218448392, 0.017513586392048416, 0.704], [573, 232, 47, 79, 0, 53, 0.26452633336603054, 0.014399079628740144, 0.62], [610, 180, 28, 21, 0, 54, 0.2882346149763522, 0.06452346256646052, 0.591], [457, 169, 47, 34, 0, 0, 0.2519543179138836, 0.13640149027142318, 1.071], [575, 78, 21, 35, 0, 49, 0.45575277048357143, 0.29444230564158685, 1.06], [167, 83, 34, 38, 0, 52, -0.18925497069639197, 0.20967963810245008, 0.778], [609, 68, 61, 35, 0, 0, 0.47633545878571454, 0.28841897271482786, 0.98], [343, 105, 82, 146, 2, 0, 0.05040080471371451, 0.2492337280231701, 1.069], [465, 51, 31, 30, 0, 43, 0.2387621161763127, 0.30983111684754183, 0.961], [22, 22, 44, 25, 0, 52, -0.5717781510316639, 0.4388528044094044, 1.185]]}, 'HW/g1/rgb/89.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[19, 306, 39, 25, 0, 53, -0.3183135406497048, -0.06470746962773834, 0.653], [476, 209, 159, 321, 2, 0, 0.15702863972726103, 0.03607138753927402, 0.589], [87, 227, 55, 43, 0, 0, -0.3246007782471091, 0.027237966888305157, 0.865], [262, 262, 147, 134, 1, 1, -0.0758649840565894, -0.023120895175758646, 0.877], [361, 167, 37, 28, 0, 0, 0.08361087000002217, 0.14216337205272714, 1.088], [41, 90, 39, 37, 0, 52, -0.347395176430944, 0.19861424894676571, 0.77], [245, 106, 83, 142, 2, 0, -0.12268914005084247, 0.24792973170029683, 1.071], [356, 52, 28, 29, 0, 43, 0.06722609631467626, 0.31433693731703527, 0.98]]}, 'HW/g1/rgb/123.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[401, 189, 20, 19, 0, 45, 0.08699539130594167, 0.057428927076608585, 0.609], [70, 234, 32, 26, 0, 0, -0.36580361093928854, 0.018066042448981735, 0.907], [227, 274, 145, 128, 1, 1, -0.1291093826130516, -0.041347923752081676, 0.895], [347, 183, 31, 27, 0, 0, 0.058692547273808716, 0.11380601380920082, 1.092], [21, 111, 38, 35, 0, 52, -0.39701385790655597, 0.18304779911637176, 0.82], [235, 120, 79, 133, 2, 0, -0.14416162595999352, 0.22918790462593802, 1.1], [488, 111, 101, 102, 2, 0, 0.2895529371385894, 0.22568454256908763, 1.011], [335, 69, 27, 27, 0, 43, 0.03391340183094929, 0.292359490640064, 0.999]]}, 'HW/g1/rgb/145.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[484, 411, 110, 138, 1, 2, 0.18914640524481913, -0.18431167139675025, 0.676], [397, 405, 41, 69, 0, 54, 0.10382207351150018, -0.20020240348979987, 0.762], [335, 337, 32, 58, 0, 52, 0.0, -0.0, 0.0], [190, 327, 138, 116, 1, 1, -0.1837502495091895, -0.11975472546800689, 0.895], [42, 278, 34, 28, 0, 0, -0.430633006676876, -0.05059248117979594, 0.958], [443, 176, 90, 146, 2, 0, 0.23082027905953717, 0.12597786762767518, 1.088], [313, 254, 57, 106, 0, 0, -0.002598145685973525, -0.014506746762815943, 1.104], [28, 178, 57, 69, 0, 0, -0.4144816056085033, 0.09864725637496928, 0.877], [205, 158, 80, 127, 2, 0, -0.20601557743022958, 0.16606256255620033, 1.141], [443, 176, 90, 146, 2, 0, 0.23082027905953717, 0.12597786762767518, 1.088], [321, 135, 25, 26, 0, 52, 0.011454302102403204, 0.19383795481213997, 1.056]]}, 'HW/g1/rgb/159.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[26, 379, 53, 22, 0, 53, -0.37739906665938866, -0.17426698075793726, 0.793], [424, 330, 152, 229, 2, 0, 0.11715862741847671, -0.08991839599470827, 0.648], [256, 332, 128, 99, 1, 1, -0.09139223891972743, -0.13468124099395012, 0.948], [424, 330, 152, 229, 2, 0, 0.11715862741847671, -0.08991839599470827, 0.648], [519, 193, 103, 159, 2, 0, 0.3628700717277235, 0.09426529574447844, 1.075], [71, 186, 34, 29, 0, 52, -0.36752032278784796, 0.09082213791101712, 0.915], [358, 128, 25, 24, 0, 43, 0.07563682741537484, 0.2052759070819294, 1.052], [268, 176, 75, 135, 2, 0, -0.0904703993979955, 0.1367461963862908, 1.181], [596, 119, 88, 24, 0, 0, 0.8195652523683483, 0.3704498289815253, 1.764]]}, 'HW/g1/rgb/171.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[577, 130, 124, 41, 0, 18, 0.43628829988718454, 0.19316608969432977, 1.007], [577, 130, 124, 41, 0, 18, 0.43628829988718454, 0.19316608969432977, 1.007], [335, 318, 96, 64, 1, 1, 0.02983971992933376, -0.10453751843582486, 0.879], [63, 362, 127, 92, 0, 54, -0.31612471048373664, -0.14604245271454944, 0.762], [577, 130, 124, 41, 0, 18, 0.43628829988718454, 0.19316608969432977, 1.007], [335, 318, 96, 64, 1, 1, 0.02983971992933376, -0.10453751843582486, 0.879], [577, 130, 124, 41, 0, 18, 0.43628829988718454, 0.19316608969432977, 1.007], [151, 188, 26, 27, 0, 52, -0.2567152316120036, 0.09134754567832183, 0.952], [577, 130, 124, 41, 0, 18, 0.43628829988718454, 0.19316608969432977, 1.007], [577, 130, 124, 41, 0, 18, 0.43628829988718454, 0.19316608969432977, 1.007], [342, 167, 41, 27, 0, 53, 0.05227672548067404, 0.15013392875788922, 1.149], [454, 152, 26, 24, 0, 52, 0.24273763637905416, 0.1638536123623612, 1.054], [45, 94, 61, 50, 0, 54, -0.6268330289539427, 0.35437368891132615, 1.41]]}, 'HW/g1/rgb/186.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[581, 451, 71, 56, 0, 53, 0.29074458215091326, -0.22392542201253884, 0.661], [403, 334, 97, 73, 1, 1, 0.0, -0.0, 0.0], [122, 374, 161, 130, 0, 0, -0.24257751381893175, -0.16157984146022933, 0.764], [523, 366, 36, 96, 0, 53, 0.2398751219347129, -0.13819313596621985, 0.697], [268, 297, 47, 31, 0, 0, -0.0812778101280891, -0.08935340947875053, 1.061], [583, 339, 72, 64, 0, 53, 0.26722289534592597, -0.0926445217339194, 0.603], [519, 263, 61, 49, 0, 0, 0.36995869638473017, -0.030706140896463077, 1.096], [21, 240, 26, 23, 0, 53, -0.512244709347727, 0.010580921152124674, 1.058], [226, 199, 28, 27, 0, 52, -0.1425504816257505, 0.07598232435812963, 0.977], [420, 200, 97, 142, 2, 0, 0.20677565644331927, 0.09035221709615744, 1.187], [506, 147, 28, 25, 0, 43, 0.32210907585018933, 0.1668342386926184, 1.019], [133, 107, 64, 46, 0, 52, -0.453529587711116, 0.3482087346993625, 1.515], [26, 101, 52, 19, 0, 54, -0.8466493563518946, 0.42653008236307854, 1.779]]}, 'HW/g1/rgb/199.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[454, 344, 146, 142, 1, 1, 0.20450761015616706, -0.14377074898555672, 0.888], [176, 302, 68, 33, 0, 53, -0.17541735439892506, -0.07102531363980158, 0.768], [587, 365, 29, 26, 0, 54, 0.3143793381616425, -0.13743427388197865, 0.699], [329, 280, 58, 38, 0, 0, 0.025441881328494686, -0.05937113637117996, 1.058], [540, 183, 200, 145, 2, 0, 0.522202581521274, 0.14621779979332306, 1.403], [294, 177, 27, 26, 0, 52, -0.033333289289464295, 0.11288005881102536, 0.989], [97, 211, 24, 22, 0, 53, -0.3903316177037823, 0.06303423930802868, 1.088], [540, 183, 200, 145, 2, 0, 0.522202581521274, 0.14621779979332306, 1.403], [32, 104, 65, 24, 0, 29, -0.7623970558822031, 0.3841320527150337, 1.636], [181, 88, 25, 37, 0, 53, -0.4682757361049818, 0.5556711903860408, 2.127], [15, 33, 30, 23, 0, 0, -0.8329909661056244, 0.593737837954812, 1.686]]}, 'HW/g1/rgb/213.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[541, 365, 20, 25, 0, 54, 0.2770260341564233, -0.1456921272482778, 0.741], [425, 318, 94, 83, 1, 1, 0.16876660583107256, -0.11000819630618658, 0.925], [604, 300, 69, 25, 0, 53, 0.3024514650998329, -0.056447784053953516, 0.633], [142, 333, 177, 151, 2, 0, -0.2148044160155593, -0.10850992202752055, 0.755], [288, 274, 59, 41, 0, 0, -0.04587165629990813, -0.0486011349577541, 1.052], [542, 251, 61, 51, 0, 0, 0.41380577258015366, -0.009015875163323323, 1.102], [48, 195, 26, 23, 0, 53, -0.45939488558027225, 0.0881800096690031, 1.045], [259, 169, 26, 28, 0, 52, -0.08843719206638324, 0.12315615414796098, 0.967], [544, 135, 31, 26, 0, 43, 0.38335005107007464, 0.18576137336163412, 1.012], [544, 135, 31, 26, 0, 43, 0.38335005107007464, 0.18576137336163412, 1.012], [437, 181, 79, 142, 2, 0, 0.23885818230012526, 0.1269856363750641, 1.181], [169, 69, 28, 42, 0, 53, -0.3405007562318885, 0.41527339060885965, 1.419]]}, 'HW/g1/rgb/234.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[81, 460, 53, 40, 0, 0, -0.28309349499581754, -0.2599283700294398, 0.735], [573, 333, 56, 87, 0, 53, 0.2850057914330781, -0.09600612968792546, 0.668], [627, 382, 25, 35, 0, 0, 0.32544279174749485, -0.14179541134979948, 0.631], [423, 318, 83, 48, 1, 1, 0.16875959882749314, -0.11202996856262461, 0.942], [160, 319, 111, 102, 0, 54, -0.19136122344200016, -0.09055611411602824, 0.751], [411, 285, 62, 42, 1, 1, 0.12891407920685585, -0.05208426383868503, 0.809], [160, 319, 111, 102, 0, 54, -0.19136122344200016, -0.09055611411602824, 0.751], [268, 259, 61, 46, 0, 0, -0.07783058915187421, -0.021747303468986904, 1.016], [56, 258, 62, 46, 0, 0, -0.4409100709462134, -0.020423459257369308, 1.034], [518, 230, 59, 45, 0, 0, 0.3728529087182086, 0.029448505143265993, 1.11], [110, 31, 69, 43, 0, 52, -0.45435689270772506, 0.478809383785675, 1.347], [243, 147, 27, 30, 0, 52, -0.10878086073671026, 0.1511167834281519, 0.923], [110, 31, 69, 43, 0, 52, -0.45435689270772506, 0.478809383785675, 1.347], [528, 120, 32, 26, 0, 43, 0.35733751266680863, 0.211269577537001, 1.014], [528, 118, 31, 27, 0, 43, 0.35804232038410017, 0.21504503945536393, 1.016], [415, 156, 77, 135, 2, 0, 0.19366370532073438, 0.17370457689687346, 1.167], [110, 31, 69, 43, 0, 52, -0.45435689270772506, 0.478809383785675, 1.347]]}, 'HW/g1/rgb/258.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[205, 417, 46, 28, 0, 53, -0.14047337356767625, -0.21983789619683522, 0.778], [169, 383, 26, 30, 0, 0, -0.1646113592495247, -0.15528866877379102, 0.686], [107, 412, 96, 135, 2, 0, -0.2084365475219583, -0.1670507297302778, 0.609], [426, 373, 95, 81, 1, 1, 0.17158158995815898, -0.19556995514668102, 0.932], [597, 401, 84, 123, 0, 54, 0.3105266197854593, -0.1705766441517311, 0.666], [29, 338, 59, 40, 0, 0, -0.4525954156374937, -0.1460589381933391, 0.961], [256, 326, 63, 46, 0, 0, -0.09486283027110949, -0.13003681142023873, 0.984], [498, 282, 56, 43, 0, 0, 0.34106860551000884, -0.06690945216683429, 1.126], [74, 106, 112, 79, 0, 8, -0.5117577753710311, 0.2986268477062399, 1.29], [233, 215, 28, 28, 0, 52, -0.12065176976792574, 0.04608909213569251, 0.898], [622, 196, 34, 37, 0, 41, 0.5562277038487561, 0.09067191884009897, 1.096], [506, 174, 30, 25, 0, 43, 0.3287472412995063, 0.123858117871645, 1.04], [392, 211, 77, 126, 2, 0, 0.14822315658783822, 0.06708975102821434, 1.158], [74, 106, 112, 79, 0, 8, -0.5117577753710311, 0.2986268477062399, 1.29]]}, 'HW/g1/rgb/270.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[183, 407, 71, 34, 0, 53, -0.16286019086168943, -0.1997950455338013, 0.751], [89, 382, 60, 26, 0, 53, -0.21834107639321804, -0.1319079341716835, 0.587], [404, 346, 118, 85, 1, 1, 0.13745369905052565, -0.15364516927666194, 0.93], [227, 307, 51, 33, 0, 0, -0.1373319913381286, -0.09590974857262653, 0.952], [461, 244, 56, 48, 0, 0, 0.2718411971445876, 0.003809398969157555, 1.124], [193, 201, 32, 31, 0, 52, -0.1719066836638403, 0.06389113806204486, 0.858], [348, 175, 77, 120, 2, 0, 0.062488704459663307, 0.13247391351589227, 1.128], [460, 134, 28, 24, 0, 43, 0.24932922463118465, 0.19224964157396923, 1.038], [47, 68, 33, 33, 0, 29, -1.0440267542333979, 0.6963258055543702, 2.366]]}, 'HW/g1/rgb/295.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[108, 408, 185, 133, 2, 0, -0.20743167698172738, -0.16302418880100628, 0.609], [57, 381, 87, 72, 0, 53, -0.2331943529045891, -0.12246128998867746, 0.549], [359, 304, 127, 111, 1, 1, 0.06060368272542387, -0.07892827373286776, 0.824], [180, 263, 43, 29, 0, 0, -0.20450686409599522, -0.025831260863630433, 0.922], [553, 247, 77, 70, 0, 54, 0.2657169237425392, -0.00105951324882439, 0.675], [408, 175, 34, 33, 0, 0, 0.17478058640248942, 0.13294367916665784, 1.132], [136, 158, 36, 35, 0, 52, -0.23906040247368093, 0.11817949237128367, 0.812], [294, 118, 81, 123, 2, 0, -0.036872212823735026, 0.23155440272063793, 1.094], [405, 70, 27, 25, 0, 43, 0.15154200606798293, 0.2950732003260262, 1.014], [518, 73, 61, 47, 0, 52, 0.37016568054726656, 0.3152165396651583, 1.102]]}, 'HW/g1/rgb/307.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[33, 407, 66, 19, 0, 54, -0.24704102907962883, -0.14153257553126802, 0.532], [623, 312, 33, 24, 0, 53, 0.42361867623868854, -0.09069647915721482, 0.832], [309, 336, 124, 133, 1, 1, -0.006956891056079997, -0.11552478368077278, 0.777], [68, 189, 40, 38, 0, 52, -0.3130909752299563, 0.07261128710835568, 0.77], [232, 135, 83, 125, 2, 0, -0.14539054739345228, 0.19622421751342578, 1.069], [340, 83, 24, 25, 0, 43, 0.042872678104241765, 0.27382328060679856, 1.016], [465, 81, 88, 48, 0, 54, 0.26981858290059896, 0.2962792248494187, 1.086]]}, 'HW/g1/rgb/326.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[39, 409, 78, 31, 0, 45, -0.2267768544902691, -0.13440292478264163, 0.499], [377, 349, 94, 76, 1, 1, 0.09436937189347212, -0.15553414946821997, 0.914], [164, 333, 50, 31, 0, 0, -0.2156931154800033, -0.124894201644921, 0.869], [377, 349, 94, 76, 1, 1, 0.09436937189347212, -0.15553414946821997, 0.914], [568, 356, 144, 194, 2, 0, 0.2736370058736125, -0.11885741953233461, 0.654], [403, 259, 54, 44, 0, 0, 0.16281070193506336, -0.023844976441389186, 1.114], [125, 215, 35, 36, 0, 52, -0.2459845103889483, 0.04039211081379734, 0.787], [286, 207, 81, 146, 2, 0, -0.052251394079094064, 0.07190603462217392, 1.114], [505, 160, 51, 55, 0, 34, 0.35879096759613327, 0.1622905730649862, 1.141], [447, 152, 34, 44, 0, 53, 0.22990743415525502, 0.16338723585658602, 1.051]]}, 'HW/g1/rgb/381.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[608, 455, 64, 29, 0, 0, 0.3182551078635024, -0.22691425514122304, 0.657], [573, 400, 83, 33, 0, 0, 0.3178582554156335, -0.18957877614472107, 0.745], [616, 358, 42, 44, 0, 54, 0.38465007703312976, -0.14303981315942849, 0.773], [360, 256, 87, 62, 1, 1, 0.06316648159513466, -0.013814666880950502, 0.84], [91, 339, 183, 168, 2, 0, -0.24073555490340193, -0.10032648871019796, 0.653], [498, 280, 41, 53, 0, 53, 0.24050486036851423, -0.04455641047137701, 0.794], [180, 255, 51, 34, 0, 0, -0.2182589525709971, -0.014556410975811652, 0.984], [360, 256, 87, 62, 1, 1, 0.06316648159513466, -0.013814666880950502, 0.84], [403, 196, 39, 28, 0, 0, 0.17552572084740672, 0.0993585534005099, 1.201], [536, 221, 84, 65, 0, 52, 0.259213327083938, 0.02935725639817326, 0.709], [144, 143, 31, 31, 0, 52, -0.25168213808276635, 0.15245002011180894, 0.895], [520, 115, 69, 53, 0, 52, 0.3883881092027449, 0.248026805807899, 1.145], [298, 141, 76, 123, 2, 0, -0.03249757459947103, 0.2081956559198885, 1.199], [398, 94, 24, 23, 0, 43, 0.15362001407121048, 0.27998034712568604, 1.114]]}, 'HW/g1/rgb/394.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[39, 370, 78, 73, 1, 2, -0.34630052729776567, -0.15611872262622392, 0.762], [197, 316, 160, 171, 2, 0, -0.13446756984797012, -0.0802416103692391, 0.694], [451, 244, 86, 71, 1, 1, 0.1899711135330763, 0.002857049226868166, 0.843], [600, 321, 80, 155, 0, 0, 0.35811677815606713, -0.0941537962691409, 0.76], [271, 236, 52, 29, 0, 0, -0.07251466711193945, 0.016811947473826652, 1.012], [451, 244, 86, 71, 1, 1, 0.1899711135330763, 0.002857049226868166, 0.843], [488, 189, 41, 32, 0, 0, 0.34196459638325977, 0.11259464520438532, 1.194], [616, 208, 45, 54, 0, 54, 0.35529127425828544, 0.04490679863887393, 0.714], [608, 149, 64, 38, 0, 0, 0.5619116059690453, 0.18608445022728312, 1.16], [241, 129, 27, 30, 0, 52, -0.1120691346406977, 0.1789655384885141, 0.925], [382, 134, 74, 122, 2, 0, 0.13480221167362605, 0.22392082530147284, 1.209], [609, 95, 61, 42, 0, 54, 0.5482718341941694, 0.2816344444918613, 1.128], [488, 83, 25, 24, 0, 43, 0.31618837052522514, 0.2975402576672299, 1.104], [72, 49, 24, 26, 0, 53, -0.5160148623887582, 0.4201668947215657, 1.29]]}, 'HW/g1/rgb/411.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[92, 359, 136, 79, 1, 2, -0.27856123262675825, -0.14170375759413734, 0.759], [255, 324, 105, 204, 2, 0, -0.06128459315248088, -0.08052835517888413, 0.625], [317, 334, 59, 99, 0, 53, 0.0031468358810630226, -0.10772263969232715, 0.741], [347, 237, 60, 35, 0, 0, 0.05433897920679543, 0.015124221215976012, 1.011], [515, 277, 138, 138, 1, 1, 0.2875985221402017, -0.0444559472187845, 0.869], [575, 200, 58, 47, 0, 0, 0.48971925054791315, 0.08669854698611906, 1.139], [61, 149, 43, 42, 0, 0, -0.37760018869038037, 0.14485711944416954, 0.903], [61, 149, 43, 42, 0, 0, -0.37760018869038037, 0.14485711944416954, 0.903], [159, 30, 70, 19, 0, 52, -0.34262867211561837, 0.4771076012549123, 1.336], [323, 126, 27, 30, 0, 52, 0.013184952224151603, 0.18494146853230808, 0.932], [131, 85, 26, 19, 0, 0, -0.3880095191154756, 0.3412751178434975, 1.282], [461, 142, 77, 142, 2, 0, 0.2878034026708713, 0.20466589780739414, 1.19], [579, 83, 29, 25, 0, 43, 0.46187580233192144, 0.28514274693109537, 1.058], [24, 21, 49, 42, 0, 53, -0.8012443703576435, 0.6219720464519732, 1.672]]}, 'HW/g2/rgb/1.jpg': {'poses': [1.0, 0.0, 0.0, -0.0, 0.0, 1.0, 0.0, -0.0, 0.0, 0.0, 1.0, -0.0], 'objects': [[510, 148, 259, 238, 1, 1, 0.23783229638891973, 0.11944600339968767, 0.737], [510, 148, 259, 238, 1, 1, 0.23783229638891973, 0.11944600339968767, 0.737], [484, 352, 117, 87, 1, 2, 0.16256517965568035, -0.10174904724908085, 0.581], [484, 352, 117, 87, 1, 2, 0.16256517965568035, -0.10174904724908085, 0.581], [553, 294, 45, 30, 0, 0, 0.3133491426652758, -0.06308891094277119, 0.796], [169, 214, 32, 57, 1, 1, -0.22316117216043435, 0.04926869134623213, 0.93], [510, 148, 259, 238, 1, 1, 0.23783229638891973, 0.11944600339968767, 0.737], [173, 110, 23, 31, 0, 53, -0.19485402579426506, 0.18777643219838566, 0.835], [510, 148, 259, 238, 1, 1, 0.23783229638891973, 0.11944600339968767, 0.737]]}, 'HW/g2/rgb/47.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[545, 461, 101, 38, 0, 0, 0.2183808439785552, -0.2039404620843219, 0.574], [356, 402, 191, 156, 1, 2, 0.038140519949959185, -0.143322354414363, 0.556], [356, 402, 191, 156, 1, 2, 0.038140519949959185, -0.143322354414363, 0.556], [618, 384, 43, 61, 0, 53, 0.3005441364887274, -0.1368127519461845, 0.6], [506, 221, 33, 52, 0, 0, 0.27722243328814133, 0.036313559747810926, 0.877], [115, 308, 200, 238, 2, 0, -0.19644886234887698, -0.06113188398164786, 0.597], [39, 117, 73, 80, 1, 2, -0.37538613588970393, 0.17619524515876422, 0.826], [498, 145, 32, 29, 0, 53, 0.25565000270910077, 0.14097277576357026, 0.844], [338, 149, 151, 219, 2, 0, 0.0308845755886453, 0.12737159782798518, 0.794]]}, 'HW/g2/rgb/64.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[302, 394, 231, 172, 1, 2, -0.013040393800530018, -0.15553416086844274, 0.636], [93, 351, 186, 94, 1, 2, -0.2506375176821699, -0.11900351833070069, 0.686], [20, 205, 33, 26, 0, 45, -0.27788519334419254, 0.03881218496165339, 0.572], [542, 267, 196, 336, 2, 0, 0.1922582173875124, -0.01772967811924976, 0.512], [20, 205, 33, 26, 0, 45, -0.27788519334419254, 0.03881218496165339, 0.572], [622, 150, 35, 33, 0, 0, 0.40245307404385355, 0.1259004055856249, 0.793], [368, 122, 29, 29, 0, 53, 0.07566908364112009, 0.17552004174297503, 0.856], [213, 122, 140, 206, 2, 0, -0.1280279256928903, 0.15686078496889708, 0.765]]}, 'HW/g2/rgb/108.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[580, 410, 119, 140, 1, 2, 0.2743167134196365, -0.16964441625663695, 0.626], [416, 318, 141, 255, 2, 0, 0.09016884364903187, -0.06398314552727392, 0.538], [308, 207, 25, 40, 0, 0, -0.0, 0.0, 0.0], [540, 209, 67, 32, 0, 54, 0.32344550487668355, 0.05321907601295267, 0.869], [544, 137, 84, 56, 0, 0, 0.31440765058118775, 0.14960982939314393, 0.83], [296, 142, 28, 27, 0, 53, -0.02693791405684922, 0.15238150038432874, 0.886], [285, 110, 55, 44, 0, 0, -0.04233938204706101, 0.19609706452334408, 0.872], [142, 152, 127, 214, 2, 0, -0.22163263586241153, 0.12110243266629923, 0.779]]}, 'HW/g2/rgb/126.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[463, 396, 179, 151, 1, 2, 0.15518100479238922, -0.1568931203388348, 0.633], [114, 403, 228, 33, 0, 0, -0.21859937918286673, -0.17148121404705186, 0.661], [463, 396, 179, 151, 1, 2, 0.15518100479238922, -0.1568931203388348, 0.633], [224, 316, 27, 141, 0, 0, -0.0, -0.0, 0.0], [286, 231, 47, 36, 0, 53, -0.025234515273386538, 0.013383962328014271, 0.538], [197, 211, 37, 51, 0, 0, -0.18077556580425952, 0.05405417764190327, 0.933], [286, 231, 47, 36, 0, 53, -0.025234515273386538, 0.013383962328014271, 0.538], [33, 174, 66, 33, 0, 54, -0.3738120834757542, 0.09587094700641752, 0.805], [406, 176, 109, 159, 2, 0, 0.12828368253206054, 0.09830442060284579, 0.849], [544, 117, 40, 35, 0, 43, 0.3064527582170854, 0.17256895076687684, 0.809], [173, 138, 26, 26, 0, 53, -0.20325491792431719, 0.15556049119625362, 0.871], [18, 81, 37, 30, 0, 53, -0.35118337993859505, 0.19588258143819764, 0.718]]}, 'HW/g2/rgb/147.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[436, 391, 274, 107, 1, 2, 0.13239649875411577, -0.15813057028252475, 0.66], [605, 367, 69, 95, 1, 1, 0.32459202049973546, -0.13534680077969452, 0.677], [436, 391, 274, 107, 1, 2, 0.13239649875411577, -0.15813057028252475, 0.66], [185, 340, 148, 248, 2, 0, -0.11724318278454068, -0.08525545674689901, 0.549], [606, 218, 67, 184, 1, 1, 0.38584740451023986, 0.03718502016780119, 0.802], [339, 195, 106, 157, 2, 0, 0.03519521619127823, 0.07324425683511454, 0.868], [104, 172, 20, 23, 0, 53, -0.3065868202347471, 0.10807938272737114, 0.883], [459, 129, 34, 32, 0, 43, 0.19656646189624566, 0.1594244364481466, 0.824]]}, 'HW/g2/rgb/185.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[318, 382, 127, 117, 1, 2, 0.00391545913618559, -0.14921101923338648, 0.664], [537, 393, 144, 151, 1, 1, 0.22769758056150696, -0.15059653431209796, 0.62], [235, 382, 33, 31, 0, 0, -0.08793859183559606, -0.15078402696626855, 0.671], [318, 382, 127, 117, 1, 2, 0.00391545913618559, -0.14921101923338648, 0.664], [537, 393, 144, 151, 1, 1, 0.22769758056150696, -0.15059653431209796, 0.62], [77, 243, 21, 29, 0, 53, -0.21781931204147967, 0.002803394481908805, 0.556], [580, 201, 112, 115, 0, 53, 0.32690138691860837, 0.05555103612387583, 0.746], [281, 178, 101, 154, 2, 0, -0.04616429316539191, 0.09414795163722836, 0.837], [76, 133, 66, 74, 0, 0, -0.34384155164402136, 0.1633196011092316, 0.874], [76, 133, 66, 74, 0, 0, -0.34384155164402136, 0.1633196011092316, 0.874], [402, 120, 22, 31, 0, 43, 0.11776718128839193, 0.16980740206376316, 0.815]]}, 'HW/g2/rgb/205.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[531, 401, 137, 157, 1, 1, 0.23049330408981566, -0.165198101318118, 0.645], [292, 382, 155, 139, 1, 2, -0.02630991589299515, -0.15977264258273763, 0.711], [97, 251, 32, 40, 0, 53, -0.22027905631445066, -0.005023364201706462, 0.614], [97, 251, 32, 40, 0, 53, -0.22027905631445066, -0.005023364201706462, 0.614], [566, 210, 119, 162, 2, 0, 0.326687745417843, 0.04689639593558854, 0.787], [403, 115, 33, 32, 0, 43, 0.12276569984331527, 0.18195853002500886, 0.84], [289, 160, 102, 145, 2, 0, -0.036626018606789025, 0.1241714901715451, 0.873], [63, 129, 25, 26, 0, 53, -0.3878958061709892, 0.1809003010667683, 0.935]]}, 'HW/g2/rgb/236.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[254, 301, 72, 96, 0, 52, -0.06271468576344374, -0.057130777857377565, 0.629], [254, 301, 72, 96, 0, 52, -0.06271468576344374, -0.057130777857377565, 0.629], [518, 360, 244, 97, 1, 2, 0.2636842642736881, -0.14785545591962623, 0.785], [518, 360, 244, 97, 1, 2, 0.2636842642736881, -0.14785545591962623, 0.785], [24, 319, 48, 40, 0, 52, -0.3737862493295227, -0.09405295474101469, 0.78], [254, 301, 72, 96, 0, 52, -0.06271468576344374, -0.057130777857377565, 0.629], [551, 113, 41, 35, 0, 43, 0.31579689921762377, 0.17791783682891404, 0.809], [418, 165, 105, 159, 2, 0, 0.15005033045994084, 0.11762629817514716, 0.878], [201, 115, 25, 25, 0, 53, -0.17798646899831957, 0.2060030500640279, 0.951], [63, 131, 117, 181, 2, 0, -0.35346227471409924, 0.16202517653446746, 0.852]]}, 'HW/g2/rgb/257.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[111, 465, 53, 29, 0, 0, -0.21582951235984432, -0.23270729277295488, 0.643], [621, 437, 37, 46, 0, 0, 0.34094761485328456, -0.21273234072703356, 0.674], [545, 359, 189, 91, 1, 2, 0.2689812834370009, -0.13199546326621225, 0.707], [391, 232, 21, 31, 0, 53, 0.07555684428632516, 0.013888144545139687, 0.598], [93, 331, 103, 114, 1, 2, -0.2546564866245954, -0.09786987407394628, 0.697], [545, 359, 189, 91, 1, 2, 0.2689812834370009, -0.13199546326621225, 0.707], [93, 331, 103, 114, 1, 2, -0.2546564866245954, -0.09786987407394628, 0.697], [625, 247, 30, 43, 1, 1, 0.456086845915814, -0.001396987839190677, 0.89], [391, 232, 21, 31, 0, 53, 0.07555684428632516, 0.013888144545139687, 0.598], [307, 163, 39, 69, 0, 0, -0.012388384745486527, 0.13878663216939077, 1.011], [629, 141, 22, 55, 0, 0, 0.0, 0.0, 0.0], [517, 155, 118, 163, 2, 0, 0.2824441970557614, 0.12717253465390285, 0.845], [329, 88, 60, 75, 0, 0, 0.021618384985176486, 0.234860554845816, 0.899], [294, 101, 25, 26, 0, 53, -0.031176231135242136, 0.22177646216180308, 0.925], [153, 111, 121, 186, 2, 0, -0.22027871873430813, 0.1846104022795603, 0.827]]}, 'HW/g2/rgb/286.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[72, 443, 145, 73, 0, 0, -0.266007661619011, -0.2164868975120981, 0.665], [224, 326, 92, 113, 1, 2, -0.10325082506721894, -0.09144865193374514, 0.692], [224, 326, 92, 113, 1, 2, -0.10325082506721894, -0.09144865193374514, 0.692], [224, 326, 92, 113, 1, 2, -0.10325082506721894, -0.09144865193374514, 0.692], [560, 242, 27, 43, 0, 53, 0.22691476524382811, 0.0037492043804700265, 0.56], [71, 237, 143, 174, 2, 0, -0.27312985737238976, 0.010172572133396328, 0.68], [560, 242, 27, 43, 0, 53, 0.22691476524382811, 0.0037492043804700265, 0.56], [619, 190, 41, 84, 0, 0, 0.44777822273723117, 0.08254887033376523, 0.891], [435, 158, 41, 68, 0, 0, 0.19278324957948512, 0.141029468113022, 0.969], [430, 94, 19, 25, 0, 53, 0.1687699896893034, 0.22242603878476855, 0.885], [23, 103, 47, 151, 0, 0, -0.0, 0.0, 0.0], [279, 105, 128, 193, 2, 0, -0.04746508613631269, 0.18931504878841357, 0.812]]}, 'HW/g2/rgb/313.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[609, 462, 62, 35, 0, 0, 0.31982523661326545, -0.23487303662689807, 0.658], [378, 231, 38, 26, 0, 53, 0.1044792042776802, 0.024777744384204863, 0.996], [190, 295, 53, 91, 0, 53, -0.1780016383513601, -0.0701492797127386, 0.867], [378, 231, 38, 26, 0, 53, 0.1044792042776802, 0.024777744384204863, 0.996], [80, 119, 123, 150, 1, 2, -0.3806222713866387, 0.2066454828332318, 0.984], [509, 189, 52, 88, 0, 0, 0.33325324009461105, 0.09788378703697818, 1.038], [511, 116, 29, 27, 0, 53, 0.291593603460559, 0.1932529652433438, 0.899], [80, 119, 123, 150, 1, 2, -0.3806222713866387, 0.2066454828332318, 0.984], [389, 133, 38, 169, 0, 53, 0.10311520486340996, 0.1565924779514143, 0.838]]}, 'HW/g2/rgb/329.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[53, 468, 35, 23, 0, 0, -0.2864244756077198, -0.2436000148102009, 0.664], [332, 367, 146, 193, 1, 2, 0.021892922237527916, -0.1509406714751394, 0.755], [514, 465, 53, 29, 0, 0, 0.21832814522069302, -0.23994546673167821, 0.663], [332, 367, 146, 193, 1, 2, 0.021892922237527916, -0.1509406714751394, 0.755], [332, 367, 146, 193, 1, 2, 0.021892922237527916, -0.1509406714751394, 0.755], [332, 367, 146, 193, 1, 2, 0.021892922237527916, -0.1509406714751394, 0.755], [332, 367, 146, 193, 1, 2, 0.021892922237527916, -0.1509406714751394, 0.755], [332, 367, 146, 193, 1, 2, 0.021892922237527916, -0.1509406714751394, 0.755], [332, 367, 146, 193, 1, 2, 0.021892922237527916, -0.1509406714751394, 0.755], [598, 247, 65, 39, 0, 54, 0.29337719968409265, -0.0009841700844635444, 0.627], [127, 244, 155, 188, 2, 0, -0.224522517663035, 0.0024605192629967836, 0.726], [627, 162, 26, 38, 0, 0, 0.0, 0.0, 0.0], [464, 161, 39, 63, 0, 0, 0.2561801646690154, 0.14592458571035544, 1.038], [42, 78, 85, 74, 1, 2, -0.42029421841636644, 0.2597203356366828, 0.935], [42, 78, 85, 74, 1, 2, -0.42029421841636644, 0.2597203356366828, 0.935], [496, 91, 60, 73, 0, 0, 0.27503535144892577, 0.23527206419798372, 0.918], [457, 102, 26, 25, 0, 53, 0.22301838784534236, 0.22572392494133678, 0.948], [42, 78, 85, 74, 1, 2, -0.42029421841636644, 0.2597203356366828, 0.935], [343, 122, 33, 159, 0, 53, 0.04059409996118634, 0.17654527563166061, 0.861]]}, 'HW/g2/rgb/359.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[457, 451, 32, 57, 0, 0, 0.17596809505096636, -0.2533982082683495, 0.748], [402, 440, 25, 48, 0, 0, 0.10924170436076602, -0.2423625718781524, 0.756], [531, 257, 172, 247, 2, 0, 0.381296675137726, -0.019311593966033592, 1.067], [91, 382, 93, 32, 0, 0, -0.3067258524956055, -0.18696320482255654, 0.832], [91, 382, 93, 32, 0, 0, -0.3067258524956055, -0.18696320482255654, 0.832], [531, 257, 172, 247, 2, 0, 0.381296675137726, -0.019311593966033592, 1.067], [285, 330, 284, 116, 1, 2, -0.04384456650056891, -0.12530294997410737, 0.903], [531, 257, 172, 247, 2, 0, 0.381296675137726, -0.019311593966033592, 1.067], [85, 266, 144, 169, 2, 0, -0.3104200878715471, -0.02703978385177612, 0.82], [13, 173, 26, 55, 0, 0, -0.6222017524760073, 0.15105484167965838, 1.251], [383, 168, 39, 56, 0, 0, 0.1343078251520168, 0.1531371639998721, 1.187], [531, 257, 172, 247, 2, 0, 0.381296675137726, -0.019311593966033592, 1.067], [17, 109, 34, 65, 0, 0, -0.4966525367757914, 0.22925330477829745, 1.012], [376, 117, 21, 20, 0, 53, 0.10799945213723881, 0.22675005521037092, 1.063], [259, 114, 46, 95, 0, 54, -0.08706536385439177, 0.20779325806966786, 0.952]]}, 'HW/g2/rgb/383.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[427, 466, 47, 28, 0, 0, 0.13058256785290367, -0.25558382543454267, 0.703], [350, 458, 28, 43, 0, 0, 0.04232115352504367, -0.25259382635564037, 0.721], [454, 334, 133, 194, 1, 2, 0.19529555564462803, -0.12327773071402622, 0.848], [334, 240, 122, 181, 2, 0, 0.024578257059221748, 0.007610662567832587, 0.761], [76, 280, 153, 70, 1, 2, -0.3465954313482641, -0.049438536051993894, 0.881], [627, 317, 25, 49, 1, 1, 0.45180330518352696, -0.10273276717607503, 0.876], [76, 280, 153, 70, 1, 2, -0.3465954313482641, -0.049438536051993894, 0.881], [454, 334, 133, 194, 1, 2, 0.19529555564462803, -0.12327773071402622, 0.848], [334, 240, 122, 181, 2, 0, 0.024578257059221748, 0.007610662567832587, 0.761], [259, 135, 44, 64, 0, 0, -0.10535640668094468, 0.21145958706778906, 1.152], [571, 117, 124, 131, 1, 1, 0.4263187961038095, 0.21480461486062422, 1.007], [428, 116, 103, 136, 2, 0, 0.1903988936394946, 0.21840379609258878, 1.016], [255, 68, 92, 68, 0, 0, -0.09962423462867293, 0.2990139553859848, 1.016], [107, 105, 122, 180, 2, 0, -0.32651636508365883, 0.22242186766520514, 0.954]]}, 'HW/g2/rgb/415.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[394, 459, 35, 41, 0, 0, 0.09545461954053247, -0.2558975365631879, 0.727], [394, 459, 35, 41, 0, 0, 0.09545461954053247, -0.2558975365631879, 0.727], [369, 328, 127, 119, 1, 2, 0.07699146401619257, -0.11581581532728677, 0.855], [257, 259, 25, 77, 0, 53, -0.07599373583387029, -0.017166670651700293, 0.802], [369, 328, 127, 119, 1, 2, 0.07699146401619257, -0.11581581532728677, 0.855], [574, 338, 105, 143, 1, 1, 0.3473556876110706, -0.123260976976897, 0.811], [257, 259, 25, 77, 0, 53, -0.07599373583387029, -0.017166670651700293, 0.802], [585, 177, 108, 136, 2, 0, 0.42636527097956856, 0.10899945011580306, 0.955], [473, 134, 116, 126, 1, 1, 0.2621753076239333, 0.18558202394712636, 1.002], [339, 137, 89, 133, 2, 0, 0.040182556734512355, 0.17863053123928394, 0.991], [154, 96, 97, 65, 0, 0, -0.26497346430744606, 0.24827101754655703, 1.001], [13, 92, 27, 24, 0, 54, -0.4312141641860099, 0.22076830242712758, 0.867]]}, 'HW/g3/rgb/1.jpg': {'poses': [1.0, 0.0, 0.0, -0.0, 0.0, 1.0, 0.0, -0.0, 0.0, 0.0, 1.0, -0.0], 'objects': [[115, 453, 61, 53, 1, 2, -0.1763761980217723, -0.18335140722634902, 0.536], [99, 386, 61, 43, 1, 2, -0.20616715413695802, -0.13416972732382193, 0.58], [616, 234, 43, 27, 0, 53, 0.458793494210279, 0.018364819336230603, 0.922], [494, 139, 175, 91, 2, 0, 0.2417830129108291, 0.14438870603133327, 0.816], [494, 139, 175, 91, 2, 0, 0.2417830129108291, 0.14438870603133327, 0.816], [193, 154, 103, 156, 0, 0, -0.1676991774203197, 0.12735203792097466, 0.837], [494, 139, 175, 91, 2, 0, 0.2417830129108291, 0.14438870603133327, 0.816], [193, 154, 103, 156, 0, 0, -0.1676991774203197, 0.12735203792097466, 0.837], [603, 78, 74, 43, 0, 0, 0.38187742336115016, 0.2227761595514648, 0.802], [36, 54, 72, 109, 0, 0, -0.7189809309517685, 0.4968031684447483, 1.565]]}, 'HW/g3/rgb/65.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[494, 334, 292, 292, 1, 2, 0.20030063324475547, -0.09827328533335108, 0.676], [16, 358, 33, 25, 1, 2, -0.2870770180232749, -0.10788125623796482, 0.583], [558, 371, 163, 91, 1, 2, 0.0, -0.0, 0.0], [301, 316, 118, 328, 2, 0, -0.009326748774912397, -0.04867682703955282, 0.421], [498, 110, 49, 60, 0, 0, 0.2498948486196779, 0.18552761265109963, 0.825], [498, 110, 49, 60, 0, 0, 0.2498948486196779, 0.18552761265109963, 0.825], [590, 102, 99, 59, 0, 0, 0.37877005615794723, 0.19834180324486658, 0.833], [400, 119, 110, 92, 2, 0, 0.11239483502381895, 0.16716443530005334, 0.796], [115, 156, 30, 25, 0, 53, -0.4116541487411141, 0.1862077341028181, 1.251], [529, 57, 48, 43, 0, 0, 0.28005662886749316, 0.24717746994149667, 0.791], [529, 57, 48, 43, 0, 0, 0.28005662886749316, 0.24717746994149667, 0.791]]}, 'HW/g3/rgb/77.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[605, 465, 70, 29, 0, 0, 0.24020768429891795, -0.18131625766601928, 0.501], [244, 273, 244, 408, 2, 0, -0.047644345220411034, -0.018263805089684948, 0.41], [613, 347, 54, 58, 1, 1, 0.2817996469910753, -0.0954455163680464, 0.572], [244, 273, 244, 408, 2, 0, -0.047644345220411034, -0.018263805089684948, 0.41], [26, 135, 31, 20, 0, 53, -0.5915599493790923, 0.22816342597678974, 1.243], [26, 135, 31, 20, 0, 53, -0.5915599493790923, 0.22816342597678974, 1.243], [597, 115, 86, 153, 0, 0, 0.41403549304727916, 0.19235616031215222, 0.888], [427, 116, 63, 164, 0, 0, 0.0, 0.0, 0.0], [427, 116, 63, 164, 0, 0, 0.0, 0.0, 0.0]]}, 'HW/g3/rgb/136.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[418, 383, 316, 154, 1, 2, 0.10134378811246575, -0.13423641484381646, 0.593], [96, 313, 192, 285, 2, 0, -0.15461619534691295, -0.04747447964288449, 0.429], [579, 349, 121, 222, 1, 2, 0.21303912243665188, -0.08304230299834939, 0.488], [22, 98, 44, 32, 0, 0, -0.3937307774192723, 0.19968917948359413, 0.816], [222, 123, 153, 148, 2, 0, -0.12154765026294392, 0.16210489551796708, 0.797], [556, 102, 122, 178, 2, 0, 0.3451917594189593, 0.20619928164472323, 0.866], [366, 101, 81, 179, 0, 0, 0.0, 0.0, 0.0], [366, 101, 81, 179, 0, 0, 0.0, 0.0, 0.0]]}, 'HW/g3/rgb/153.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[124, 345, 206, 93, 1, 2, -0.0, -0.0, 0.0], [313, 440, 31, 33, 0, 0, -0.0012308244508733275, -0.1676661707569758, 0.523], [124, 345, 206, 93, 1, 2, -0.0, -0.0, 0.0], [401, 358, 113, 222, 1, 2, 0.08142425787255624, -0.10547567076439099, 0.57], [124, 345, 206, 93, 1, 2, -0.0, -0.0, 0.0], [9, 328, 19, 72, 0, 53, -0.2514778593954536, -0.06759308988107146, 0.499], [83, 258, 166, 27, 0, 0, -0.287159584969788, -0.014853424914450407, 0.752], [93, 87, 186, 103, 2, 0, -0.2787703002791482, 0.20059222487655728, 0.763], [590, 293, 99, 173, 0, 0, 0.25690886161973614, -0.04384653933122258, 0.565], [93, 87, 186, 103, 2, 0, -0.2787703002791482, 0.20059222487655728, 0.763], [554, 155, 19, 67, 0, 29, 0.4174418007604416, 0.15892804330712593, 1.056], [93, 87, 186, 103, 2, 0, -0.2787703002791482, 0.20059222487655728, 0.763], [442, 100, 110, 150, 2, 0, 0.17787330955635808, 0.20399252221050923, 0.845], [276, 174, 27, 34, 0, 53, -0.05883950662541314, 0.11051955133162168, 0.928], [615, 47, 50, 75, 0, 0, 0.0, 0.0, 0.0]]}, 'HW/g3/rgb/189.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[123, 400, 247, 120, 1, 2, -0.18983174046935322, -0.15293536169527164, 0.601], [295, 375, 110, 210, 1, 2, -0.015962891966096158, -0.1061461455090134, 0.498], [210, 137, 20, 51, 0, 29, -0.20142672728338423, 0.21071553079588587, 1.169], [390, 135, 30, 19, 0, 53, 0.11509736548508917, 0.1694246517912928, 0.923], [59, 112, 21, 68, 0, 53, -0.3359051818753302, 0.17659614524166303, 0.797], [365, 73, 83, 62, 0, 0, 0.06984627816885734, 0.23941582912861842, 0.837], [463, 143, 33, 29, 0, 54, 0.29442715127276375, 0.20457259682042744, 1.201]]}, 'HW/g3/rgb/208.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[572, 449, 28, 25, 0, 0, 0.2409780292933356, -0.1902068379242434, 0.567], [388, 231, 184, 443, 2, 0, 0.058635783051986336, 0.01201571339113549, 0.483], [623, 456, 34, 47, 0, 0, 0.28461879809786883, -0.1939910641926367, 0.559], [146, 401, 113, 157, 1, 1, -0.14562432929613534, -0.13420744975301366, 0.524], [146, 401, 113, 157, 1, 1, -0.14562432929613534, -0.13420744975301366, 0.524], [521, 160, 26, 55, 0, 52, 0.23450729231519754, 0.09785794414435628, 0.688], [66, 130, 75, 94, 1, 2, -0.32383026164273826, 0.15154042786347616, 0.79], [558, 119, 41, 37, 0, 0, 0.2656591193871511, 0.13881368308207948, 0.661], [66, 130, 75, 94, 1, 2, -0.32383026164273826, 0.15154042786347616, 0.79], [14, 97, 29, 28, 0, 0, -0.36930654069583, 0.1835456970363557, 0.745], [388, 231, 184, 443, 2, 0, 0.058635783051986336, 0.01201571339113549, 0.483], [239, 150, 106, 159, 2, 0, -0.10155600517005198, 0.1295519936417023, 0.816]]}, 'HW/g3/rgb/225.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[156, 420, 106, 119, 1, 1, -0.15057140434346403, -0.16561542770962362, 0.576], [370, 348, 58, 41, 0, 0, 0.04474888247074826, -0.08223567246736233, 0.488], [317, 368, 23, 107, 0, 0, 0.0, -0.0, 0.0], [452, 326, 30, 24, 0, 29, 0.18069303108496768, -0.10519237997003053, 0.796], [601, 328, 71, 32, 0, 53, 0.3362008440712084, -0.09630999379848057, 0.711], [529, 95, 221, 103, 1, 1, 0.7109402158861267, 0.501349259343668, 2.008], [529, 95, 221, 103, 1, 1, 0.7109402158861267, 0.501349259343668, 2.008], [630, 254, 20, 22, 0, 54, 0.48529920255755227, -0.012246637665710563, 0.932], [529, 95, 221, 103, 1, 1, 0.7109402158861267, 0.501349259343668, 2.008], [317, 173, 168, 122, 2, 0, 0.0057331018076046975, 0.16300882195646588, 1.35], [74, 198, 148, 86, 0, 0, -0.3352211784407141, 0.0671132716551015, 0.845], [529, 95, 221, 103, 1, 1, 0.7109402158861267, 0.501349259343668, 2.008], [529, 95, 221, 103, 1, 1, 0.7109402158861267, 0.501349259343668, 2.008], [72, 157, 98, 67, 0, 0, -0.32480935524005555, 0.11952167268104084, 0.812], [317, 173, 168, 122, 2, 0, 0.0057331018076046975, 0.16300882195646588, 1.35], [317, 173, 168, 122, 2, 0, 0.0057331018076046975, 0.16300882195646588, 1.35], [529, 95, 221, 103, 1, 1, 0.7109402158861267, 0.501349259343668, 2.008]]}, 'HW/g3/rgb/250.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[287, 415, 22, 130, 0, 53, -0.028781715563753803, -0.17761071285996582, 0.636], [133, 394, 103, 171, 1, 1, -0.18230991347595354, -0.14893129554855603, 0.609], [362, 108, 31, 41, 0, 29, 0.10715012466899118, 0.31147637097010955, 1.365], [362, 108, 31, 41, 0, 29, 0.10715012466899118, 0.31147637097010955, 1.365], [554, 237, 29, 33, 0, 29, 0.3427292057379762, 0.012970029470080316, 0.867], [444, 184, 21, 68, 0, 53, 0.2088836019237234, 0.10020603425898097, 0.977], [533, 146, 26, 33, 0, 0, 0.24668735466278544, 0.11311747384783849, 0.684], [490, 143, 23, 44, 0, 0, 0.0, 0.0, 0.0], [69, 146, 138, 132, 0, 0, -0.3583910358372743, 0.14635813502242262, 0.885], [572, 100, 136, 44, 0, 0, 0.2758284673921954, 0.1566759135084266, 0.649], [405, 114, 21, 21, 0, 53, 0.14451786969205077, 0.21106731150563954, 0.967], [362, 108, 31, 41, 0, 29, 0.10715012466899118, 0.31147637097010955, 1.365], [466, 41, 36, 60, 0, 53, 0.0, 0.0, 0.0], [444, 22, 26, 22, 0, 53, 0.16868900912775614, 0.2921981963536034, 0.789]]}, 'HW/g3/rgb/278.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[455, 457, 25, 45, 0, 0, 0.1447376714261573, -0.21757959388370987, 0.624], [258, 199, 183, 294, 2, 0, -0.09152245864693609, 0.07644895071038016, 0.983], [487, 414, 23, 39, 0, 53, 0.18964514888674086, -0.18488772435108772, 0.666], [111, 401, 103, 158, 1, 1, -0.20978762865459205, -0.16007567957181973, 0.625], [258, 199, 183, 294, 2, 0, -0.09152245864693609, 0.07644895071038016, 0.983], [408, 103, 19, 143, 0, 54, 0.13201183866972477, 0.20216686581746107, 0.855], [587, 223, 105, 41, 0, 0, 0.4236700093680504, 0.03589086385137661, 0.942], [45, 112, 91, 79, 0, 0, -0.37565525494048335, 0.18723179765270423, 0.845], [408, 103, 19, 143, 0, 54, 0.13201183866972477, 0.20216686581746107, 0.855], [532, 118, 102, 39, 0, 0, 0.24878976753498214, 0.1466793428568575, 0.693], [258, 199, 183, 294, 2, 0, -0.09152245864693609, 0.07644895071038016, 0.983], [45, 112, 91, 79, 0, 0, -0.37565525494048335, 0.18723179765270423, 0.845], [408, 103, 19, 143, 0, 54, 0.13201183866972477, 0.20216686581746107, 0.855], [258, 199, 183, 294, 2, 0, -0.09152245864693609, 0.07644895071038016, 0.983]]}, 'HW/g3/rgb/292.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[479, 249, 29, 22, 0, 54, 0.14962526549691255, -0.002686407161276934, 0.551], [479, 249, 29, 22, 0, 54, 0.14962526549691255, -0.002686407161276934, 0.551], [628, 398, 23, 34, 0, 54, 0.34148877274108835, -0.16576711342424658, 0.66], [260, 384, 97, 191, 1, 1, -0.05361369270176305, -0.13612868818645357, 0.597], [51, 413, 102, 133, 1, 2, -0.2629706488568299, -0.1669535399053701, 0.605], [627, 257, 23, 44, 0, 54, 0.41621605854235877, -0.014605863477590542, 0.807], [479, 249, 29, 22, 0, 54, 0.14962526549691255, -0.002686407161276934, 0.551], [350, 110, 102, 165, 2, 0, 0.05147801892019876, 0.1972214742969871, 0.877], [350, 110, 102, 165, 2, 0, 0.05147801892019876, 0.1972214742969871, 0.877], [56, 124, 28, 66, 0, 53, -0.3654351361710879, 0.17289196407376167, 0.857], [198, 178, 20, 38, 0, 53, -0.18999409154182478, 0.1112453096406438, 0.989], [161, 82, 110, 81, 0, 0, -0.21796948525272286, 0.23347223359722327, 0.861], [512, 89, 92, 179, 2, 0, 0.28851293731583905, 0.2297402603003787, 0.885], [512, 89, 92, 179, 2, 0, 0.28851293731583905, 0.2297402603003787, 0.885]]}, 'HW/g3/rgb/307.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[435, 236, 410, 390, 2, 0, 0.18124410770579044, 0.015134075245707588, 0.911], [565, 440, 20, 19, 0, 53, 0.26667854374049677, -0.2067775910865189, 0.645], [158, 411, 119, 138, 1, 2, -0.15899491031466456, -0.16795264730828127, 0.616], [39, 356, 49, 67, 0, 53, -0.28403914640564765, -0.1135869834980262, 0.625], [376, 382, 80, 166, 1, 1, 0.06360080624450752, -0.14067183439774086, 0.626], [10, 302, 21, 40, 0, 53, -0.2913418951660578, -0.053638908738391826, 0.58], [448, 170, 30, 44, 0, 0, 0.20717709079636887, 0.11816370558490862, 0.94], [529, 82, 25, 24, 0, 54, 0.4627484373322548, 0.3544113929286537, 1.307], [143, 142, 129, 151, 2, 0, -0.24269313058651326, 0.14756583220062536, 0.858], [331, 119, 90, 154, 0, 0, 0.03246114333721336, 0.2492766139461851, 1.187]]}, 'HW/g3/rgb/327.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[502, 395, 233, 170, 1, 2, 0.19591538743061832, -0.15584681475253395, 0.633], [502, 395, 233, 170, 1, 2, 0.19591538743061832, -0.15584681475253395, 0.633], [415, 150, 449, 220, 2, 0, 0.1725880493003974, 0.1651152860139343, 1.04], [49, 113, 99, 125, 0, 0, -0.3827790475526749, 0.19221284226016175, 0.874], [415, 150, 449, 220, 2, 0, 0.1725880493003974, 0.1651152860139343, 1.04], [49, 113, 99, 125, 0, 0, -0.3827790475526749, 0.19221284226016175, 0.874], [274, 133, 39, 38, 0, 0, -0.0557651360683636, 0.15621874888709109, 0.836], [49, 113, 99, 125, 0, 0, -0.3827790475526749, 0.19221284226016175, 0.874], [438, 57, 47, 33, 0, 0, 0.16577136921579289, 0.254052190976532, 0.813]]}, 'HW/g3/rgb/340.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[406, 368, 467, 217, 2, 0, 0.10833851634333028, -0.14452880937183057, 0.717], [433, 409, 120, 91, 1, 2, 0.11582518746675309, -0.15945196687640048, 0.592], [486, 116, 96, 112, 0, 0, 0.23667364157730494, 0.17971021017067343, 0.836], [386, 142, 35, 23, 0, 0, 0.09861272286382368, 0.14361010476401184, 0.835], [321, 172, 22, 28, 0, 54, 0.010445542542248375, 0.11787139928251236, 0.963], [105, 143, 34, 79, 0, 53, -0.43678835598269156, 0.21530371555455477, 1.264], [486, 116, 96, 112, 0, 0, 0.23667364157730494, 0.17971021017067343, 0.836], [486, 116, 96, 112, 0, 0, 0.23667364157730494, 0.17971021017067343, 0.836]]}, 'HW/g3/rgb/356.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[476, 379, 328, 202, 1, 2, 0.1927533217704749, -0.15888401902646737, 0.723], [12, 385, 25, 31, 0, 0, -0.33084598026848094, -0.15227398442931342, 0.663], [345, 182, 166, 169, 2, 0, 0.0447975455748299, 0.09401335278836838, 0.888], [79, 160, 24, 37, 0, 54, -0.4657650808792472, 0.17054022533296972, 1.199], [584, 197, 112, 80, 0, 0, 0.47060448075198996, 0.08577938298967715, 1.058], [345, 182, 166, 169, 2, 0, 0.0447975455748299, 0.09401335278836838, 0.888], [79, 160, 24, 37, 0, 54, -0.4657650808792472, 0.17054022533296972, 1.199], [509, 211, 32, 58, 0, 29, 0.32073216459972687, 0.057877945835221184, 0.999], [509, 211, 32, 58, 0, 29, 0.32073216459972687, 0.057877945835221184, 0.999], [79, 160, 24, 37, 0, 54, -0.4657650808792472, 0.17054022533296972, 1.199]]}, 'HW/g3/rgb/376.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[357, 409, 178, 141, 1, 2, 0.05078936993164002, -0.19473610143857695, 0.723], [76, 321, 21, 23, 0, 52, -0.3218105140100795, -0.10133921756336481, 0.818], [589, 397, 86, 163, 1, 2, 0.32710637138206644, -0.18014575911208205, 0.722], [76, 321, 21, 23, 0, 52, -0.3218105140100795, -0.10133921756336481, 0.818], [76, 321, 21, 23, 0, 52, -0.3218105140100795, -0.10133921756336481, 0.818], [172, 121, 241, 143, 0, 0, -0.3019854707601415, 0.2656091262617045, 1.285], [172, 121, 241, 143, 0, 0, -0.3019854707601415, 0.2656091262617045, 1.285], [629, 199, 19, 22, 0, 29, 0.5823822515024432, 0.08725912787085102, 1.122], [172, 121, 241, 143, 0, 0, -0.3019854707601415, 0.2656091262617045, 1.285], [465, 196, 71, 56, 0, 29, 0.28248962132410776, 0.09406384281130704, 1.137], [172, 121, 241, 143, 0, 0, -0.3019854707601415, 0.2656091262617045, 1.285]]}, 'HW/g3/rgb/409.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[340, 303, 235, 354, 2, 0, 0.041269172427114614, -0.09206286476299089, 0.978], [623, 377, 33, 104, 0, 0, 0.39001431009475407, -0.1658012617322669, 0.766], [68, 345, 137, 155, 1, 2, -0.35253230587580797, -0.14180385659447906, 0.867], [194, 165, 388, 156, 2, 0, -0.26249248390703495, 0.1769753301701246, 1.321], [539, 184, 109, 134, 2, 0, 0.3775947600149615, 0.10451376551678775, 1.019], [393, 140, 89, 69, 0, 0, 0.1305568505303493, 0.1765210703356417, 1.007]]}, 'HW/g3/rgb/420.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[17, 387, 35, 33, 0, 0, -0.2841519948549241, -0.13489544749758553, 0.579], [59, 378, 26, 47, 0, 0, -0.24613378446071874, -0.12737254021277256, 0.584], [233, 347, 274, 265, 1, 2, -0.12011434540370336, -0.14917533502278582, 0.894], [517, 318, 24, 25, 0, 53, 0.33492199461523414, -0.11916563535005292, 1.002], [233, 347, 274, 265, 1, 2, -0.12011434540370336, -0.14917533502278582, 0.894], [536, 196, 25, 90, 0, 38, 0.42592880966542707, 0.09638027869408329, 1.165], [321, 209, 47, 25, 0, 29, 0.013428433714749211, 0.07581727975147917, 1.238], [458, 223, 31, 55, 0, 29, 0.2797806687833237, 0.04499693228075984, 1.181], [321, 209, 47, 25, 0, 29, 0.013428433714749211, 0.07581727975147917, 1.238]]}, 'HW/g3/rgb/441.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[132, 437, 32, 36, 0, 36, -0.22876721814827625, -0.23987622989991916, 0.76], [337, 426, 85, 107, 1, 1, 0.0258124597285622, -0.20612895029069053, 0.693], [541, 432, 62, 87, 0, 53, 0.26244571656924315, -0.21576809676912642, 0.702], [25, 424, 50, 94, 0, 53, -0.32760829101664196, -0.20177901777900054, 0.686], [560, 299, 20, 22, 0, 29, 0.44572543172894813, -0.09627429117601571, 1.1], [419, 214, 42, 24, 0, 52, 0.17272273249774864, 0.053030064556535875, 1.001], [158, 233, 317, 162, 2, 0, -0.3221195585595802, 0.026921095426220534, 1.248], [117, 232, 36, 29, 0, 0, -0.3081689632925907, 0.02197020859481964, 0.946], [409, 202, 34, 25, 0, 0, 0.15558169550230858, 0.0725938276671704, 0.997], [543, 198, 114, 163, 2, 0, 0.3684798141778423, 0.07759723835151972, 0.977]]}, 'HW/g4/rgb/1.jpg': {'poses': [1.0, 0.0, 0.0, -0.0, 0.0, 1.0, 0.0, -0.0, 0.0, 0.0, 1.0, -0.0], 'objects': [[490, 262, 300, 177, 2, 0, 0.16107463280606243, -0.014658172996262039, 0.556], [206, 312, 126, 157, 1, 1, -0.12201457516364883, -0.07434495046300542, 0.682], [583, 141, 53, 37, 0, 52, 0.3846591594697575, 0.1507204581638559, 0.868], [583, 141, 53, 37, 0, 52, 0.3846591594697575, 0.1507204581638559, 0.868], [421, 78, 30, 30, 0, 53, 0.1468349585588148, 0.23194275963275948, 0.835], [234, 128, 24, 90, 0, 53, -0.11943545159501583, 0.17561627031723995, 0.9], [421, 78, 30, 30, 0, 53, 0.1468349585588148, 0.23194275963275948, 0.835], [421, 78, 30, 30, 0, 53, 0.1468349585588148, 0.23194275963275948, 0.835], [542, 69, 41, 45, 0, 39, 0.3236847331797572, 0.252266147078814, 0.862], [29, 60, 58, 120, 0, 0, -0.6014197146400411, 0.3927139270972904, 1.277]]}, 'HW/g4/rgb/38.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[580, 378, 101, 95, 0, 0, 0.2756313302571108, -0.13718720512642796, 0.629], [107, 326, 128, 174, 1, 2, -0.22315374217457606, -0.08616260268901997, 0.652], [581, 209, 118, 67, 1, 2, 0.2388416158970437, 0.03325426729002681, 0.543], [351, 305, 188, 207, 2, 0, 0.032889599409597306, -0.05310462090375115, 0.545], [581, 209, 118, 67, 1, 2, 0.2388416158970437, 0.03325426729002681, 0.543], [513, 171, 34, 34, 0, 53, 0.28767964369584187, 0.10891865547588514, 0.878], [364, 130, 44, 42, 0, 0, 0.07051015812084684, 0.16535170736495755, 0.862], [364, 130, 44, 42, 0, 0, 0.07051015812084684, 0.16535170736495755, 0.862], [496, 103, 78, 50, 0, 0, 0.25196593743850393, 0.19885653117249677, 0.841], [181, 147, 111, 144, 2, 0, -0.18691401032117044, 0.13900124499512567, 0.849], [353, 96, 28, 26, 0, 53, 0.05270050780029677, 0.20536303948906018, 0.828], [288, 99, 23, 55, 0, 0, -0.0350142015293025, 0.1951805528442194, 0.803], [288, 99, 23, 55, 0, 0, -0.0350142015293025, 0.1951805528442194, 0.803], [14, 13, 28, 26, 0, 0, -0.6954860088540262, 0.5404584718132519, 1.403]]}, 'HW/g4/rgb/62.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[616, 364, 47, 55, 0, 53, 0.3353870011905944, -0.13140480919911215, 0.674], [480, 361, 98, 99, 0, 0, 0.1751226356549488, -0.12179245546371818, 0.641], [615, 305, 50, 40, 0, 0, 0.3506414526258841, -0.06888984766780194, 0.707], [39, 316, 78, 163, 1, 1, -0.3103979791920918, -0.07896976928269496, 0.683], [252, 295, 175, 199, 2, 0, -0.0574770412268742, -0.04514797933068989, 0.558], [436, 138, 97, 146, 2, 0, 0.16569622419833277, 0.14752349681757232, 0.826], [436, 138, 97, 146, 2, 0, 0.16569622419833277, 0.14752349681757232, 0.826], [436, 138, 97, 146, 2, 0, 0.16569622419833277, 0.14752349681757232, 0.826], [436, 138, 97, 146, 2, 0, 0.16569622419833277, 0.14752349681757232, 0.826], [121, 128, 105, 133, 2, 0, -0.2751158067067667, 0.16820136112606762, 0.862], [615, 84, 25, 44, 0, 10, 0.0, 0.0, 0.0], [280, 86, 29, 29, 0, 53, -0.04726134886631685, 0.2201075208480313, 0.832]]}, 'HW/g4/rgb/78.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[368, 373, 42, 27, 0, 53, 0.05666341426864249, -0.13450680391526024, 0.641], [546, 302, 44, 26, 0, 53, 0.30377305026822116, -0.07352229732245087, 0.795], [122, 242, 133, 146, 2, 0, -0.17209033048411126, 0.0036286942396692038, 0.542], [346, 150, 168, 204, 2, 0, 0.04542921200214231, 0.1384428167347603, 0.872], [122, 242, 133, 146, 2, 0, -0.17209033048411126, 0.0036286942396692038, 0.542], [286, 181, 34, 50, 1, 2, -0.025328323880350802, 0.05806286506565166, 0.54], [334, 176, 34, 42, 0, 0, 0.01724676645154325, 0.06183104900108322, 0.534], [25, 148, 50, 71, 0, 53, -0.4298067957944282, 0.1458635048300121, 0.9], [126, 91, 30, 25, 0, 29, -0.25432413454300895, 0.20964329903480466, 0.818], [584, 82, 52, 70, 1, 2, 0.3905394462195153, 0.23808202218160515, 0.878], [525, 70, 64, 52, 0, 5, 0.3026322227497192, 0.25346031310056094, 0.871], [160, 62, 33, 27, 0, 53, -0.20537569386717994, 0.24520346613180452, 0.806], [90, 66, 30, 52, 0, 0, -0.2925456221636269, 0.23511264025426734, 0.79], [115, 51, 41, 28, 0, 0, -0.2569959154011272, 0.25179823335892837, 0.781]]}, 'HW/g4/rgb/95.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[284, 394, 42, 48, 0, 53, -0.031779366803585354, -0.15480050916623309, 0.633], [437, 360, 121, 133, 1, 2, 0.1251932386484145, -0.11658920664235493, 0.619], [599, 323, 76, 107, 1, 2, 0.34136739616338957, -0.0924689016046772, 0.727], [15, 254, 31, 50, 0, 54, -0.2672883230504999, -0.007108831520546581, 0.541], [258, 176, 130, 205, 2, 0, -0.08128088138227386, 0.10108334415345628, 0.873], [629, 157, 22, 24, 0, 53, 0.4873947719793174, 0.1382153333097258, 0.939], [258, 176, 130, 205, 2, 0, -0.08128088138227386, 0.10108334415345628, 0.873], [84, 95, 33, 30, 0, 53, -0.30492931543138047, 0.20024009262630565, 0.802], [461, 88, 52, 51, 0, 29, 0.2092016330338686, 0.22597817568590753, 0.865], [516, 98, 53, 71, 1, 2, 0.2883671945418367, 0.21216975320131873, 0.867]]}, 'HW/g4/rgb/116.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[145, 404, 50, 54, 0, 53, -0.17248793946599897, -0.16108628692423704, 0.617], [317, 312, 89, 38, 0, 53, 0.002976966197874736, -0.07641614409760528, 0.701], [468, 306, 172, 115, 1, 2, 0.1859969468736272, -0.07273397045445626, 0.734], [87, 181, 174, 219, 2, 0, -0.0, 0.0, 0.0], [337, 68, 43, 25, 0, 54, 0.031474067057193446, 0.24868778769798933, 0.845], [392, 76, 20, 28, 0, 43, 0.10956737654506865, 0.2406058691777019, 0.856], [557, 117, 142, 184, 2, 0, 0.3065952049520348, 0.1633965590697499, 0.766]]}, 'HW/g4/rgb/134.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[45, 412, 38, 55, 0, 53, -0.2649592094018084, -0.16348478640270206, 0.596], [601, 388, 78, 163, 1, 2, 0.251086706331662, -0.12459011179986616, 0.531], [616, 314, 47, 28, 0, 0, 0.24880341334613829, -0.056158027063321785, 0.5], [219, 360, 106, 129, 0, 0, -0.09289937231444019, -0.1111270305637955, 0.59], [400, 311, 166, 119, 1, 2, 0.10053407353889332, -0.07643836903368585, 0.712], [24, 262, 48, 59, 0, 0, -0.24679476718551824, -0.013577264555890197, 0.515], [628, 252, 24, 43, 0, 0, 0.2602558374072234, -0.004946656087663475, 0.503], [317, 195, 45, 31, 0, 0, 0.004042898459881239, 0.08033241072238369, 0.952], [59, 141, 119, 124, 2, 0, -0.34475588302888344, 0.14203840412215912, 0.818], [274, 86, 62, 56, 0, 54, -0.05609865960944234, 0.2224884916264355, 0.841], [337, 92, 55, 73, 1, 2, 0.03154856189046491, 0.21567560802281094, 0.847], [507, 133, 137, 222, 2, 0, 0.235137331267837, 0.13827975379957824, 0.74]]}, 'HW/g4/rgb/152.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[154, 363, 60, 61, 0, 53, -0.16703122475324522, -0.12197841660388976, 0.631], [506, 362, 31, 53, 0, 10, 0.1687990642826311, -0.10234471095744015, 0.534], [562, 315, 34, 37, 0, 0, 0.1907718375176016, -0.05322351624839451, 0.467], [371, 324, 111, 102, 1, 2, 0.06777105701458705, -0.0935417373757918, 0.726], [458, 142, 145, 219, 2, 0, 0.17388569931156614, 0.12623930167279604, 0.734], [622, 258, 35, 95, 0, 0, 0.43239598875833957, -0.016828614397754984, 0.852], [458, 142, 145, 219, 2, 0, 0.17388569931156614, 0.12623930167279604, 0.734], [35, 178, 57, 87, 0, 54, -0.40020248363108935, 0.09763491280897756, 0.868], [196, 103, 20, 30, 0, 52, -0.0, 0.0, 0.0], [276, 93, 61, 22, 0, 53, -0.05306968431624009, 0.21174575722549652, 0.837], [458, 142, 145, 219, 2, 0, 0.17388569931156614, 0.12623930167279604, 0.734]]}, 'HW/g4/rgb/159.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[426, 361, 207, 137, 1, 2, 0.12868619246861926, -0.13281267764296256, 0.699], [564, 396, 28, 53, 0, 53, 0.2108441972974655, -0.12690249228038455, 0.512], [564, 396, 28, 53, 0, 53, 0.2108441972974655, -0.12690249228038455, 0.512], [205, 400, 95, 114, 1, 2, -0.11086201975649515, -0.15624344772195806, 0.614], [545, 147, 189, 191, 2, 0, 0.2811558252615894, 0.12099166083792444, 0.739], [14, 318, 29, 55, 0, 0, -0.281069541710786, -0.0674320511411976, 0.567], [545, 147, 189, 191, 2, 0, 0.2811558252615894, 0.12099166083792444, 0.739], [342, 245, 37, 47, 0, 0, 0.0432682035962759, 0.0016511391092511598, 0.951], [545, 147, 189, 191, 2, 0, 0.2811558252615894, 0.12099166083792444, 0.739], [84, 171, 31, 35, 0, 53, -0.33116388246974116, 0.10805028350739859, 0.871], [277, 140, 53, 62, 0, 0, -0.05409703576033179, 0.15355755473090582, 0.876], [367, 126, 19, 46, 0, 52, 0.14009871197867685, 0.3204726090983665, 1.615]]}, 'HW/g4/rgb/190.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[399, 302, 167, 127, 1, 2, 0.09656825936951068, -0.06399676697752955, 0.692], [588, 395, 104, 160, 1, 2, 0.21712622939295798, -0.11842388293202027, 0.481], [196, 345, 90, 112, 0, 0, -0.11958929145835628, -0.10009683994904403, 0.612], [623, 304, 33, 31, 0, 0, 0.21995585112393443, -0.041379871665775336, 0.432], [399, 302, 167, 127, 1, 2, 0.09656825936951068, -0.06399676697752955, 0.692], [548, 215, 59, 20, 0, 52, 0.33376032575275233, 0.044446719141993, 0.866], [330, 182, 55, 33, 0, 0, 0.024232441250279218, 0.09983625189125156, 0.943], [101, 133, 53, 88, 0, 54, -0.31166208066594325, 0.16537511096300908, 0.885], [271, 55, 31, 19, 0, 54, -0.058971908135500156, 0.2598977892849407, 0.823], [338, 77, 63, 74, 0, 0, 0.032245986351368966, 0.2316463852163525, 0.829], [564, 72, 37, 49, 0, 53, 0.0, 0.0, 0.0], [473, 41, 42, 80, 0, 30, 0.17870831847020602, 0.23149207391944804, 0.683]]}, 'HW/g4/rgb/210.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[90, 411, 30, 55, 0, 53, -0.1995975827167024, -0.14695856639474614, 0.539], [281, 321, 57, 70, 0, 53, -0.030610732027231198, -0.06875704859128053, 0.555], [516, 307, 247, 166, 1, 2, 0.19922946889337967, -0.06034657499475136, 0.599], [40, 240, 80, 99, 0, 0, -0.20014316943038588, 0.004420384829148493, 0.442], [415, 168, 24, 26, 0, 53, 0.14421059119427435, 0.11211136943208833, 0.869], [563, 96, 35, 36, 0, 53, 0.31951077547355383, 0.19320991275601193, 0.779], [161, 64, 22, 26, 0, 29, -0.36429510949903393, 0.43301927500232795, 1.439], [327, 37, 47, 71, 0, 52, 0.015394332883427897, 0.25639509348535783, 0.742], [584, 19, 25, 39, 0, 54, 0.0, 0.0, 0.0], [412, 40, 64, 80, 2, 0, 0.11752996074120192, 0.24862862230729008, 0.73]]}, 'HW/g4/rgb/229.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[495, 429, 290, 94, 0, 0, 0.15314771125296012, -0.1554352278183192, 0.514], [211, 419, 34, 60, 0, 53, -0.08669367119814968, -0.14522391722348685, 0.508], [467, 331, 249, 108, 1, 2, 0.15558261317243355, -0.08677701890631105, 0.618], [620, 265, 38, 26, 0, 53, 0.3015157630388819, -0.0187308040173578, 0.598], [117, 228, 216, 118, 1, 2, -0.13909952148619054, 0.01273999234469996, 0.427], [18, 81, 37, 33, 0, 53, -0.3531398333087265, 0.19697384930136305, 0.722], [207, 107, 123, 132, 2, 0, -0.12496616242479225, 0.1620377280284162, 0.705], [459, 33, 52, 51, 0, 0, 0.16961013884494014, 0.2503841060414421, 0.711], [525, 46, 63, 86, 1, 2, 0.24530235276842907, 0.23345271313700133, 0.706]]}, 'HW/g4/rgb/250.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[343, 425, 65, 24, 0, 53, 0.02437531902431282, -0.152924175151005, 0.517], [58, 241, 43, 22, 0, 45, -0.1975933009275196, 0.0038984876242510787, 0.467], [553, 402, 173, 156, 1, 2, 0.1826557816541306, -0.11960714469112307, 0.464], [25, 154, 38, 55, 0, 53, -0.6074602713894585, 0.19353858092649912, 1.272], [25, 154, 38, 55, 0, 53, -0.6074602713894585, 0.19353858092649912, 1.272], [281, 184, 209, 236, 2, 0, -0.02195147990421264, 0.04082088191921641, 0.398], [281, 184, 209, 236, 2, 0, -0.02195147990421264, 0.04082088191921641, 0.398], [563, 76, 62, 73, 0, 5, 0.2928506979308311, 0.20069227872999904, 0.714], [625, 80, 30, 83, 0, 0, 0.3612822768209538, 0.19350127753115967, 0.705]]}, 'HW/g4/rgb/270.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[443, 424, 88, 112, 0, 0, 0.10331753299815681, -0.14324545431249747, 0.487], [443, 424, 88, 112, 0, 0, 0.10331753299815681, -0.14324545431249747, 0.487], [627, 386, 26, 58, 0, 0, 0.2821191871408553, -0.1265359324933286, 0.547], [171, 322, 206, 239, 2, 0, -0.10128970197272892, -0.05373090833444057, 0.428], [425, 148, 127, 178, 2, 0, 0.1253434142766993, 0.11134247535357589, 0.687], [425, 148, 127, 178, 2, 0, 0.1253434142766993, 0.11134247535357589, 0.687], [96, 120, 122, 108, 2, 0, -0.28075994446444097, 0.16230670700327793, 0.779], [258, 85, 35, 35, 0, 53, -0.06675646271602562, 0.1908691571714413, 0.717], [425, 148, 127, 178, 2, 0, 0.1253434142766993, 0.11134247535357589, 0.687]]}, 'HW/g4/rgb/297.jpg': {'poses': [0.99, 0.0, 0.0, -0.0, 0.0, 0.99, 0.0, -0.0, 0.0, 0.0, 0.99, -0.0], 'objects': [[541, 445, 89, 69, 0, 0, 0.1973950688725931, -0.17363283675778873, 0.528], [612, 409, 56, 22, 0, 0, 0.21898898453435794, -0.12012766423458551, 0.446], [278, 418, 142, 82, 0, 0, -0.03125437811836132, -0.14779487912081482, 0.52], [35, 337, 71, 199, 1, 1, -0.25588983688393385, -0.0834350795649796, 0.555], [213, 371, 44, 97, 0, 45, -0.09020529666466387, -0.11132136506671098, 0.539], [258, 329, 24, 53, 0, 0, -0.039755940836461556, -0.058545980831282055, 0.427], [354, 247, 63, 40, 0, 52, 0.0282740296736168, -0.0006796581285051271, 0.433], [354, 247, 63, 40, 0, 52, 0.0282740296736168, -0.0006796581285051271, 0.433], [561, 168, 157, 175, 2, 0, 0.2892738848525413, 0.09172748408079953, 0.711], [354, 247, 63, 40, 0, 52, 0.0282740296736168, -0.0006796581285051271, 0.433], [561, 168, 157, 175, 2, 0, 0.2892738848525413, 0.09172748408079953, 0.711], [314, 131, 32, 31, 0, 29, -0.0005169689412169441, 0.13977524032022723, 0.735], [352, 98, 35, 33, 0, 53, 0.0443905125477693, 0.17521746631158505, 0.716], [269, 107, 20, 68, 0, 53, -0.0, 0.0, 0.0], [177, 147, 114, 150, 2, 0, -0.17664446627391084, 0.12754060053145216, 0.779], [299, 94, 28, 30, 0, 54, -0.01812315189474912, 0.1789461464573505, 0.712], [54, 52, 108, 104, 0, 0, -0.6398413590863744, 0.47759969353578113, 1.489]]}}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}